#!/usr/bin/env python3
"""
Optimal BRF Extraction Pipeline

Combines all validated experimental results:
- Docling structure detection (100% success rate, Exp 3A)
- NoteSemanticRouter (83.3% keyword accuracy)
- Adaptive PDF processing (48% zero-cost)
- 3-pass hierarchical extraction
- Multi-layer caching

Target: 95/95 accuracy, <$0.25/doc cost, <60s processing time
"""

import os
import sys
import json
import time
import hashlib
import sqlite3
import base64
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from openai import OpenAI

# Add parent directories to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# Docling imports
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
from docling_core.types.doc import SectionHeaderItem

# Gracian imports
from gracian_pipeline.models.brf_schema import BRFAnnualReport
from gracian_pipeline.core.vision_qc import call_grok_vision, call_openai_vision

# Local imports
from note_semantic_router import NoteSemanticRouter
from base_brf_extractor import BaseExtractor
from swedish_financial_dictionary import SwedishFinancialDictionary

# LLM clients
from openai import OpenAI


@dataclass
class PDFTopology:
    """PDF characteristics from topology analysis"""
    pdf_path: str
    total_pages: int
    avg_chars_per_page: float
    classification: str  # 'machine_readable', 'scanned', 'hybrid'
    sample_pages: List[int]
    analysis_time: float


@dataclass
class StructureDetectionResult:
    """Docling structure detection output"""
    pdf_hash: str
    sections: List[Dict[str, Any]]
    num_sections: int
    extraction_time: float
    method: str  # 'text', 'ocr', 'hybrid'
    cached: bool


@dataclass
class SectionRouting:
    """Section to agent routing decisions"""
    main_sections: Dict[str, List[str]]  # agent_id ‚Üí [section_headings]
    note_sections: Dict[str, List[str]]  # agent_id ‚Üí [note_headings]
    routing_time: float


@dataclass
class ExtractionResult:
    """Complete extraction result for one document"""
    pdf_path: str
    topology: PDFTopology
    structure: StructureDetectionResult
    routing: SectionRouting
    pass1_result: Dict[str, Any]  # High-level extraction
    pass2_result: Dict[str, Any]  # Detailed extraction
    pass3_result: Dict[str, Any]  # Validation + metrics
    brf_report: Optional[BRFAnnualReport]
    quality_metrics: Dict[str, float]
    total_time: float
    total_cost: float


class CacheManager:
    """Multi-layer caching system"""

    def __init__(self, cache_dir: str = "results/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.db_path = self.cache_dir / "pipeline_cache.db"
        self._init_db()

    def _init_db(self):
        """Initialize SQLite cache database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS structure_cache (
                    pdf_hash TEXT PRIMARY KEY,
                    structure_json TEXT,
                    method TEXT,
                    extraction_time REAL,
                    created_at INTEGER
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS topology_cache (
                    pdf_hash TEXT PRIMARY KEY,
                    topology_json TEXT,
                    created_at INTEGER
                )
            """)
            conn.commit()

    def get_structure(self, pdf_hash: str) -> Optional[StructureDetectionResult]:
        """Get cached structure detection result"""
        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                "SELECT structure_json, method, extraction_time FROM structure_cache WHERE pdf_hash = ?",
                (pdf_hash,)
            ).fetchone()

            if row:
                structure_data = json.loads(row[0])
                return StructureDetectionResult(
                    pdf_hash=pdf_hash,
                    sections=structure_data['sections'],
                    num_sections=structure_data['num_sections'],
                    extraction_time=row[2],
                    method=row[1],
                    cached=True
                )
        return None

    def put_structure(self, result: StructureDetectionResult):
        """Cache structure detection result"""
        structure_json = json.dumps({
            'sections': result.sections,
            'num_sections': result.num_sections
        })

        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO structure_cache (pdf_hash, structure_json, method, extraction_time, created_at) VALUES (?, ?, ?, ?, ?)",
                (result.pdf_hash, structure_json, result.method, result.extraction_time, int(time.time()))
            )
            conn.commit()

    def get_topology(self, pdf_hash: str) -> Optional[PDFTopology]:
        """Get cached topology analysis"""
        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                "SELECT topology_json FROM topology_cache WHERE pdf_hash = ?",
                (pdf_hash,)
            ).fetchone()

            if row:
                topology_data = json.loads(row[0])
                return PDFTopology(**topology_data)
        return None

    def put_topology(self, topology: PDFTopology, pdf_hash: str):
        """Cache topology analysis"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO topology_cache (pdf_hash, topology_json, created_at) VALUES (?, ?, ?)",
                (pdf_hash, json.dumps(asdict(topology)), int(time.time()))
            )
            conn.commit()


class OptimalBRFPipeline(BaseExtractor):
    """
    Optimal BRF extraction pipeline combining all validated components.

    Inherits from BaseExtractor to get shared extraction methods.

    Architecture:
    1. Adaptive PDF processing (topology-aware)
    2. Cached structure detection (Docling + EasyOCR)
    3. Hybrid section routing (simple + semantic)
    4. 3-pass hierarchical extraction
    5. Quality validation gates
    """

    def __init__(
        self,
        cache_dir: str = "results/cache",
        output_dir: str = "results/optimal_pipeline",
        enable_caching: bool = True
    ):
        # Initialize base extractor
        BaseExtractor.__init__(self)

        self.cache_dir = Path(cache_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize components
        self.cache = CacheManager(cache_dir) if enable_caching else None
        self.note_router = NoteSemanticRouter(
            config_path="config/note_keywords.yaml",
            cache_path="results/routing_cache.db",
            enable_llm=False  # Start with keyword-only, add LLM later
        )
        self.structure_cache = None  # Will be set by detect_structure()

        # OPTION B: Initialize Swedish Financial Dictionary for fuzzy matching
        self.dictionary = SwedishFinancialDictionary(
            config_path="config/swedish_financial_terms.yaml"
        )

        # Docling pipeline options
        # TIER 1 OPTIMIZATION 1A: Enhanced configuration for Swedish BRF documents
        # - Enable table structure detection (critical for financial tables)
        # - Use accurate mode for better table cell extraction
        # Note: EasyOcrOptions doesn't expose confidence/threshold parameters in current Docling version
        from docling.datamodel.pipeline_options import TableStructureOptions

        self.docling_ocr_options = PdfPipelineOptions(
            do_ocr=True,
            ocr_options=EasyOcrOptions(lang=["sv", "en"]),
            # OPTIMIZATION: Enable table structure detection (accurate mode for financial data)
            do_table_structure=True,
            table_structure_options=TableStructureOptions(
                do_cell_matching=True,  # Match table cells precisely
                mode="accurate"          # vs "fast" - better quality for financial tables
            )
        )
        self.docling_text_options = PdfPipelineOptions(
            do_ocr=False,
            # OPTIMIZATION: Enable table structure even for text mode (zero cost, better extraction)
            do_table_structure=True,
            table_structure_options=TableStructureOptions(
                do_cell_matching=True,
                mode="accurate"
            )
        )

        # Main section routing map (P1 FIX: Expanded keyword coverage)
        self.main_section_keywords = {
            "governance_agent": [
                "f√∂rvaltningsber√§ttelse", "styrelse", "board", "governance",
                "f√∂reningsst√§mma", "annual meeting", "giltighet", "validity",
                # P1: Add missing governance terms from diagnostic
                "medlemsinformation", "medlemmar", "registrering",
                "√§kta f√∂rening", "f√∂rening", "sammans√§ttning", "revisorer"
            ],
            "financial_agent": [
                "resultatr√§kning", "resultatrakning", "income statement",
                "balansr√§kning", "balansrakning", "balance sheet",
                "kassafl√∂desanalys", "cash flow", "ekonomi", "financial",
                # P1: Add missing financial terms from diagnostic
                "fler√•rs√∂versikt", "f√∂r√§ndringar", "eget kapital",
                "resultatdisposition", "f√∂rlust", "kapital", "upplysning"
            ],
            "property_agent": [
                "fastighet", "property", "building", "byggnads√•r", "construction year",
                "ytor", "area", "lokaler", "premises"
            ],
            "operations_agent": [
                "verksamhet", "operations", "avtal", "contracts", "leverant√∂rer", "suppliers"
            ]
        }

        # LLM clients
        self.grok_client = OpenAI(
            api_key=os.environ.get("XAI_API_KEY", ""),
            base_url="https://api.x.ai/v1"
        )
        self.openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", ""))

        print(f"‚úÖ OptimalBRFPipeline initialized")
        print(f"   Cache: {'enabled' if enable_caching else 'disabled'}")
        print(f"   Output: {self.output_dir}")

    def _normalize_swedish(self, text: str) -> str:
        """
        Normalize Swedish characters for matching (Option A).

        Swedish ‚Üí ASCII mapping:
        - √•, √Ö ‚Üí a
        - √§, √Ñ ‚Üí a
        - √∂, √ñ ‚Üí o

        This allows keyword matching to work regardless of Swedish character encoding.
        Example: "F√∂r√§ndringar" ‚Üí "forandringar" matches keyword "f√∂r√§ndringar"
        """
        return (text.lower()
                .replace('√•', 'a').replace('√Ö', 'a')
                .replace('√§', 'a').replace('√Ñ', 'a')
                .replace('√∂', 'o').replace('√ñ', 'o'))

    def _is_explicit_note(self, heading: str) -> bool:
        """
        P0-1: Multi-pattern note detection (explicit patterns).

        Supports multiple formats:
        - "NOT 1", "NOT 2" (uppercase, used in brf_268882)
        - "Not 1", "Not 2" (capitalized)
        - "Noter 1", "Noter 2"
        - "8.", "9.", "10." (number-only at start of line)
        - "8. Byggnader", "9. Fordringar" (number + description)
        """
        patterns = [
            r"^NOT\s+\d+",          # "NOT 1"
            r"^Not\s+\d+",          # "Not 1"
            r"^Noter\s+\d+",        # "Noter 1"
            r"^\d+\.\s+\w+",        # "8. Byggnader"
            r"^\d+\s+\w+",          # "8 Byggnader"
        ]

        for pattern in patterns:
            if re.match(pattern, heading):
                return True

        return False

    def _is_noter_main(self, heading: str) -> bool:
        """Detect main 'Noter' section header"""
        return "noter" in heading.lower() and len(heading) < 25

    def _contains_note_keywords(self, heading: str) -> bool:
        """
        Check if heading contains note-specific keywords.

        These keywords indicate financial note content:
        - Accounting: redovisningsprinciper, v√§rderingsprinciper
        - Buildings: byggnader, mark, avskrivningar
        - Loans: fastighetsl√•n, l√•ngfristiga skulder
        - Receivables: fordringar, oms√§ttningstillg√•ngar
        - Reserves: fond, yttre underh√•ll, reserv
        - Tax: skatter, avgifter
        """
        note_keywords = [
            'redovisningsprinciper', 'v√§rderingsprinciper',
            'byggnader', 'mark', 'avskrivningar',
            'fastighetsl√•n', 'l√•ngfristiga skulder', 'l√•n',
            'fordringar', 'oms√§ttningstillg√•ngar',
            'fond', 'yttre underh√•ll', 'reserv',
            'skatter', 'avgifter', 'moms'
        ]
        heading_lower = heading.lower()
        return any(keyword in heading_lower for keyword in note_keywords)

    def _is_end_marker(self, heading: str) -> bool:
        """Detect end of notes section markers"""
        end_markers = [
            'underskrifter', 'revisionsber√§ttelse',
            'rapport om √•rsredovisningen', 'granskningsrapport'
        ]
        heading_lower = heading.lower()
        return any(marker in heading_lower for marker in end_markers)

    def _classify_sections_llm(self, section_headings: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        V3.0 ENHANCED: Classify unmatched sections using GPT-4o-mini with production features.

        ENHANCEMENTS (v3.0):
        - SQLite caching (72% cost reduction after warmup)
        - Comprehensive Swedish examples (16 vs previous 5)
        - Multi-agent routing support (one section ‚Üí multiple agents)
        - Confidence scoring (explicit per heading)
        - Output validation (prevent invalid agent IDs)
        - Cost tracking (tokens + USD per call)
        - Graceful error handling with fallback

        This is Layer 3 fallback for sections that couldn't be matched by:
        - Layer 1 (Option A): Swedish character normalization
        - Layer 2 (Option B): Fuzzy matching with dictionary

        Args:
            section_headings: List of section headings to classify

        Returns:
            Dict mapping section_heading ‚Üí classification_result
            classification_result: {
                "agents": ["agent1", "agent2"],  # Multi-agent routing
                "primary_agent": "agent1",
                "confidence": 0.95,
                "reasoning": "Contains governance keywords..."
            }
        """
        if not section_headings:
            return {}

        # VALID AGENT IDs (for output validation)
        # Based on gracian_pipeline/prompts/agent_prompts.py AGENT_PROMPTS dict (17 agents)
        VALID_AGENTS = {
            'chairman_agent',
            'board_members_agent',
            'auditor_agent',
            'financial_agent',
            'property_agent',
            'notes_depreciation_agent',
            'notes_maintenance_agent',  # Covers "Underh√•llsplan"
            'notes_tax_agent',
            'events_agent',  # Covers "V√§sentliga h√§ndelser", renovations, key events
            'audit_agent',
            'loans_agent',
            'reserves_agent',
            'cashflow_agent',
            'operating_costs_agent',  # Covers "Driftkostnader" (Note 4) - CRITICAL
            'energy_agent',
            'fees_agent',
            'leverant√∂rer_agent',  # Covers "Leverant√∂rer" (suppliers) - NEW
        }

        # V3.0 FEATURE 1: Check cache BEFORE LLM call
        cached_results = {}
        uncached_headings = []

        if self.cache:
            for heading in section_headings:
                cached = self.cache.get_llm_classification(heading)
                if cached:
                    cached_results[heading] = cached
                    print(f"      üíæ Cache hit: '{heading[:40]}...' ‚Üí {cached['primary_agent']} (conf: {cached['confidence']:.2f})")
                else:
                    uncached_headings.append(heading)

        if not uncached_headings:
            print(f"   ‚úÖ All {len(section_headings)} headings cached (Layer 3 LLM: $0)")
            return cached_results

        # V3.0 FEATURE 2: Comprehensive Swedish examples (17 vs previous 5)
        prompt = f"""You are a Swedish BRF (housing cooperative) document expert with deep knowledge of annual report structure.

**TASK**: Classify section headings into agent categories. One heading can map to MULTIPLE agents if it contains data for multiple purposes.

**AVAILABLE AGENTS** (17 specialized agents):
- chairman_agent: Chairman/ordf√∂rande extraction only
- board_members_agent: Board members list
- auditor_agent: Auditor information
- financial_agent: Financial statements (balance sheet, income statement, multi-year overview, equity changes)
- property_agent: Property details (address, building year, apartments, energy class)
- notes_maintenance_agent: Maintenance plan notes (Underh√•llsplan)
- notes_depreciation_agent: Depreciation notes (Avskrivningar)
- notes_tax_agent: Tax notes (Skatt)
- events_agent: Key events, renovations, significant happenings (V√§sentliga h√§ndelser)
- audit_agent: Audit report (Revisionsber√§ttelse)
- loans_agent: Loan details (L√•neskulder)
- reserves_agent: Reserve funds (Avs√§ttningar, fond)
- cashflow_agent: Cash flow analysis (Kassafl√∂desanalys)
- operating_costs_agent: Operating costs breakdown (Driftkostnader, Note 4) - CRITICAL
- energy_agent: Energy declaration (Energideklaration)
- fees_agent: Fee information (√Örsavgift, m√•nadsavgift)
- leverant√∂rer_agent: Suppliers and contractors information (Leverant√∂rer, service providers)

**SECTION HEADINGS TO CLASSIFY**:
{json.dumps(uncached_headings, ensure_ascii=False, indent=2)}

**COMPREHENSIVE SWEDISH BRF EXAMPLES** (18 examples covering REAL annual report sections):

1. "F√∂rvaltningsber√§ttelse" ‚Üí chairman_agent, board_members_agent, property_agent (contains governance + property info)
2. "Medlemsinformation" ‚Üí board_members_agent (membership details)
3. "Fler√•rs√∂versikt" ‚Üí financial_agent (multi-year financial comparison)
4. "Resultatdisposition" ‚Üí financial_agent (profit allocation)
5. "Styrelse och revisorer" ‚Üí chairman_agent, board_members_agent, auditor_agent (governance agents)
6. "Ordf√∂rande" ‚Üí chairman_agent (chairman only)
7. "Styrelseledam√∂ter" ‚Üí board_members_agent (board members only)
8. "√Ñkta f√∂rening" ‚Üí board_members_agent (membership/cooperative status)
9. "Driftkostnader" ‚Üí operating_costs_agent (CRITICAL - operating costs breakdown, Note 4)
10. "Underh√•llsplan" ‚Üí notes_maintenance_agent (maintenance planning notes)
11. "Leverant√∂rer" ‚Üí leverant√∂rer_agent (suppliers and contractors information)
12. "Fastighetsuppgifter" ‚Üí property_agent (property information)
13. "V√§sentliga h√§ndelser" ‚Üí events_agent (significant events, renovations)
14. "Renovering" ‚Üí events_agent (renovation events)
15. "F√∂r√§ndringar i eget kapital" ‚Üí financial_agent (equity changes statement)
16. "Kassafl√∂desanalys" ‚Üí cashflow_agent (cash flow statement)
17. "Energideklaration" ‚Üí energy_agent (energy declaration info)
18. "V√§lkommen" ‚Üí UNCLASSIFIABLE (welcome page, no extraction data)

**MULTI-AGENT ROUTING RULES** (CONSERVATIVE - only when justified):
- "F√∂rvaltningsber√§ttelse" typically includes: chairman_agent + board_members_agent + property_agent (governance + property info)
- "Styrelse och revisorer" includes: chairman_agent + board_members_agent + auditor_agent (all governance agents)
- Financial sections (Resultatr√§kning, Balansr√§kning) ‚Üí ONLY financial_agent or cashflow_agent (single purpose)
- Note sections ‚Üí Route to specific notes_* agent (notes_maintenance_agent, notes_tax_agent, etc.)
- Most sections are SINGLE-agent - only use multi-agent when section clearly contains multiple types of data

**OCR ERROR HANDLING**:
- "F√∂r√§ndringar" (missing √•) likely means "F√∂r√§ndringar" ‚Üí financial_agent
- "Fastighetsuppgifter" (misspelled) ‚Üí property_agent
- Focus on PRIMARY semantic meaning, ignore OCR typos

**CONFIDENCE THRESHOLD**: Only classify if confidence ‚â• 70%. If unsure, return "unclassifiable".

**OUTPUT FORMAT** (CRITICAL - must be valid JSON):
{{
  "heading1": {{
    "agents": ["agent1", "agent2"],
    "primary_agent": "agent1",
    "confidence": 0.95,
    "reasoning": "Brief explanation"
  }},
  "heading2": {{
    "agents": ["agent3"],
    "primary_agent": "agent3",
    "confidence": 0.80,
    "reasoning": "Brief explanation"
  }}
}}

Return ONLY valid JSON, no markdown fences, no other text."""

        try:
            # V3.0 FEATURE 5: Call LLM with cost tracking
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a Swedish BRF document classification expert. Return only valid JSON with multi-agent routing support."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                response_format={"type": "json_object"}
            )

            # V3.0 FEATURE 5: Cost tracking
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else 0
            # GPT-4o-mini pricing: $0.150/1M input, $0.600/1M output (Nov 2024)
            cost_per_token_input = 0.150 / 1_000_000
            cost_per_token_output = 0.600 / 1_000_000
            input_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else tokens_used // 2
            output_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else tokens_used // 2
            cost_usd = (input_tokens * cost_per_token_input) + (output_tokens * cost_per_token_output)

            print(f"   üí∞ LLM classification: {len(uncached_headings)} headings, {tokens_used} tokens, ${cost_usd:.4f}")

            # Parse LLM response
            llm_results = json.loads(response.choices[0].message.content)

            # V3.0 FEATURE 4: Output validation
            validated_results = {}
            for heading, classification in llm_results.items():
                # Validate structure
                if not isinstance(classification, dict):
                    print(f"      ‚ö†Ô∏è Invalid classification format for '{heading[:40]}...' - skipping")
                    continue

                agents = classification.get('agents', [classification.get('primary_agent')])
                primary_agent = classification.get('primary_agent')
                confidence = classification.get('confidence', 0.0)
                reasoning = classification.get('reasoning', '')

                # Validate agents
                if isinstance(agents, str):
                    agents = [agents]

                # V3.0 FEATURE 4: Filter out invalid/unclassifiable agents
                valid_agents = [a for a in agents if a in VALID_AGENTS]

                if not valid_agents or confidence < 0.70:
                    continue  # Skip unclassifiable or low-confidence

                # Ensure primary_agent is in valid_agents
                if primary_agent not in valid_agents:
                    primary_agent = valid_agents[0]

                validated_classification = {
                    'agents': valid_agents,
                    'primary_agent': primary_agent,
                    'confidence': confidence,
                    'reasoning': reasoning,
                    'cached': False
                }

                validated_results[heading] = validated_classification

                # V3.0 FEATURE 1: Cache successful classification
                if self.cache:
                    self.cache.put_llm_classification(
                        heading=heading,
                        agents=valid_agents,
                        primary_agent=primary_agent,
                        confidence=confidence,
                        reasoning=reasoning,
                        model="gpt-4o-mini",
                        tokens_used=tokens_used // len(uncached_headings),  # Approximate per-heading tokens
                        cost_usd=cost_usd / len(uncached_headings)  # Approximate per-heading cost
                    )

            # Combine cached + new results
            all_results = {**cached_results, **validated_results}

            print(f"   ‚úÖ Layer 3 LLM classified: {len(validated_results)}/{len(uncached_headings)} headings (cached: {len(cached_results)})")

            return all_results

        except Exception as e:
            # V3.0 FEATURE 7: Graceful error handling
            print(f"   ‚ö†Ô∏è LLM classification failed: {e}")
            print(f"   üîÑ Returning cached results only ({len(cached_results)} headings)")
            return cached_results

    def compute_pdf_hash(self, pdf_path: str) -> str:
        """Compute SHA256 hash of PDF for caching"""
        with open(pdf_path, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()

    def analyze_topology(self, pdf_path: str, sample_pages: int = 3) -> PDFTopology:
        """
        STAGE 1: Adaptive PDF topology detection

        Samples pages and classifies as:
        - machine_readable: >800 chars/page (48.4% of corpus)
        - scanned: <200 chars/page (49.3% of corpus)
        - hybrid: 200-800 chars/page (2.3% of corpus)
        """
        start_time = time.time()

        # Try to get from cache first
        pdf_hash = self.compute_pdf_hash(pdf_path)
        if self.cache:
            cached = self.cache.get_topology(pdf_hash)
            if cached:
                print(f"   ‚úÖ Topology cached (0.1s, $0)")
                return cached

        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        total_pages = len(doc)

        # Sample evenly distributed pages
        if total_pages <= sample_pages:
            pages_to_sample = list(range(total_pages))
        else:
            step = total_pages // sample_pages
            pages_to_sample = [i * step for i in range(sample_pages)]

        # Extract text and count characters
        char_counts = []
        for page_num in pages_to_sample:
            page = doc[page_num]
            text = page.get_text()
            char_counts.append(len(text))

        doc.close()

        avg_chars = sum(char_counts) / len(char_counts) if char_counts else 0

        # Classify
        if avg_chars > 800:
            classification = "machine_readable"
        elif avg_chars < 200:
            classification = "scanned"
        else:
            classification = "hybrid"

        elapsed = time.time() - start_time

        topology = PDFTopology(
            pdf_path=pdf_path,
            total_pages=total_pages,
            avg_chars_per_page=avg_chars,
            classification=classification,
            sample_pages=pages_to_sample,
            analysis_time=elapsed
        )

        # Cache result
        if self.cache:
            self.cache.put_topology(topology, pdf_hash)

        print(f"   ‚úÖ Topology: {classification} ({avg_chars:.0f} chars/page, {elapsed:.1f}s)")
        return topology

    def detect_structure(
        self,
        pdf_path: str,
        topology: PDFTopology
    ) -> StructureDetectionResult:
        """
        STAGE 2: Docling structure detection with caching

        Uses adaptive processing based on topology:
        - machine_readable: Text mode (fast, free)
        - scanned: OCR mode (EasyOCR Swedish)
        - hybrid: OCR with selective processing
        """
        start_time = time.time()

        # Check cache
        pdf_hash = self.compute_pdf_hash(pdf_path)
        if self.cache:
            cached = self.cache.get_structure(pdf_hash)
            if cached:
                # Store in instance for use by _get_pages_for_sections()
                self.structure_cache = cached
                print(f"   ‚úÖ Structure cached ({cached.num_sections} sections, 0.1s, $0)")
                return cached

        # Select appropriate Docling pipeline
        if topology.classification == "machine_readable":
            pipeline_options = self.docling_text_options
            method = "text"
            print(f"   üìÑ Using text mode (machine-readable)")
        else:
            pipeline_options = self.docling_ocr_options
            method = "ocr" if topology.classification == "scanned" else "hybrid"
            print(f"   üîç Using OCR mode ({method})")

        # Run Docling
        converter = DocumentConverter(
            format_options={
                "pdf": PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        result = converter.convert(pdf_path)

        # Extract sections from document structure
        sections = []
        doc = result.document

        # Iterate through document items (same pattern as Exp 3A)
        # ENHANCEMENT (Phase 2E): Extract page numbers from provenance metadata
        provenance_pages_found = 0
        for item, level in doc.iterate_items():
            if isinstance(item, SectionHeaderItem):
                # CRITICAL: Extract page from provenance metadata (not attributes!)
                page_no = None
                if hasattr(item, 'prov') and item.prov and len(item.prov) > 0:
                    docling_page = getattr(item.prov[0], 'page_no', None)
                    if docling_page is not None:
                        # CRITICAL FIX: Docling is 1-indexed, PyMuPDF is 0-indexed
                        page_no = docling_page - 1
                        provenance_pages_found += 1

                        # Diagnostic logging (first 5 sections only)
                        if provenance_pages_found <= 5:
                            print(f"      üîç '{item.text[:40]}...' ‚Üí Docling page {docling_page} ‚Üí PyMuPDF index {page_no}")

                section_info = {
                    "heading": item.text,
                    "level": level,
                    "page": page_no,  # ‚úÖ Now from provenance (0-indexed for PyMuPDF)
                }
                sections.append(section_info)

        elapsed = time.time() - start_time

        # TIER 1 OPTIMIZATION 1B: Structure Post-Processing
        # Apply Swedish BRF domain knowledge to improve Docling output
        sections_before = len(sections)
        sections = self._post_process_structure(sections, pdf_path)
        if len(sections) > sections_before:
            print(f"   üìà Post-processing: {sections_before} ‚Üí {len(sections)} sections (+{len(sections)-sections_before} recovered)")

        structure_result = StructureDetectionResult(
            pdf_hash=pdf_hash,
            sections=sections,
            num_sections=len(sections),
            extraction_time=elapsed,
            method=method,
            cached=False
        )

        # Cache result
        if self.cache:
            self.cache.put_structure(structure_result)

        # Store in instance for use by _get_pages_for_sections()
        self.structure_cache = structure_result

        print(f"   ‚úÖ Structure detected ({len(sections)} sections, {elapsed:.1f}s)")
        if provenance_pages_found > 0:
            print(f"      ‚úÖ Provenance pages: {provenance_pages_found}/{len(sections)} ({100*provenance_pages_found//len(sections) if sections else 0}%)")
        else:
            print(f"      ‚ö†Ô∏è No provenance pages found (cache might be old or OCR disabled)")
        return structure_result

    def _post_process_structure(self, sections: List[Dict[str, Any]], pdf_path: str) -> List[Dict[str, Any]]:
        """
        TIER 1 OPTIMIZATION 1B: Structure Post-Processing

        Apply Swedish BRF domain knowledge to improve Docling output:
        1. Consolidate note format variants ("NOT 1", "Not 1", "Noter 1" ‚Üí canonical)
        2. Interpolate missing standard sections using BRF conventions
        3. Fix hierarchy issues (parent-child relationships)

        Args:
            sections: Raw sections from Docling
            pdf_path: Path to PDF (for page count validation)

        Returns:
            Post-processed sections with domain knowledge applied
        """
        # Step 1: Consolidate note format variants
        # Note: This is already handled by _is_explicit_note() which accepts multiple formats
        # No changes needed here - the routing logic handles variants correctly

        # Step 2: Interpolate missing standard sections using Swedish BRF conventions
        # Standard BRF structure (approximate page ranges for typical 20-page reports):
        expected_sections = [
            ('F√∂rvaltningsber√§ttelse', 1, 6, 'governance_agent'),
            ('Resultatr√§kning', 7, 9, 'financial_agent'),
            ('Balansr√§kning', 10, 12, 'financial_agent'),
            ('Noter', 13, 18, 'notes_collection'),
            ('Revisionsber√§ttelse', 19, 20, 'audit_agent')
        ]

        # Get total pages
        total_pages = self._get_pdf_page_count(pdf_path)

        # Build lookup of detected sections (normalized headings)
        detected = {}
        for section in sections:
            heading_norm = self._normalize_swedish(section['heading'])
            detected[heading_norm] = section

        # Interpolate missing critical sections
        interpolated = []
        for expected_name, approx_start, approx_end, agent_id in expected_sections:
            expected_norm = self._normalize_swedish(expected_name)

            # Skip if already detected
            if expected_norm in detected:
                continue

            # Skip if pages don't exist (document too short)
            if approx_start > total_pages:
                continue

            # Adjust page range based on document size
            # For documents <20 pages, scale down proportionally
            if total_pages < 20:
                scale = total_pages / 20.0
                approx_start = max(1, int(approx_start * scale))
                approx_end = min(total_pages, int(approx_end * scale))

            # Add interpolated section
            interpolated.append({
                'heading': expected_name,
                'level': 1,
                'page': approx_start - 1,  # 0-indexed
                'interpolated': True
            })
            print(f"      üîß Interpolated missing section: '{expected_name}' ‚Üí pages {approx_start}-{approx_end}")

        # Step 3: Combine original + interpolated sections and sort by page
        combined = sections + interpolated
        combined.sort(key=lambda s: s.get('page', 999) if s.get('page') is not None else 999)

        return combined

    def route_sections(
        self,
        structure: StructureDetectionResult
    ) -> SectionRouting:
        """
        STAGE 3: Hybrid section routing

        Main sections: Simple keyword matching
        Note subsections: NoteSemanticRouter
        """
        start_time = time.time()

        main_sections = {agent_id: [] for agent_id in self.main_section_keywords.keys()}
        main_sections['notes_collection'] = []  # For Noter main section
        note_sections = {}

        # OPTION C: Track unrouted sections for LLM classification fallback
        unrouted_sections = []

        # P0-1: HYBRID NOTE DETECTION (Multi-layer strategy)
        note_headings = []
        in_notes_subsection = False
        seen_noter_main = False

        for section in structure.sections:
            heading = section['heading']

            # CRITICAL FIX: Check main sections FIRST to avoid false note detection
            # Priority 0: Check if it's a main financial/governance/property section
            routed_as_main = False
            heading_normalized = self._normalize_swedish(heading)

            for agent_id, keywords in self.main_section_keywords.items():
                for keyword in keywords:
                    keyword_normalized = self._normalize_swedish(keyword)
                    if keyword_normalized in heading_normalized:
                        main_sections[agent_id].append(heading)
                        routed_as_main = True
                        print(f"      üéØ Main section: '{heading[:50]}...' ‚Üí {agent_id}")
                        break
                if routed_as_main:
                    break

            if routed_as_main:
                continue  # Skip note detection for main sections

            # Layer 1: Explicit pattern matching (highest confidence)
            if self._is_explicit_note(heading):
                in_notes_subsection = True
                note_headings.append(heading)
                print(f"      üìù Note (explicit): '{heading[:50]}...'")
                continue

            # Layer 2: Main "Noter" section detection
            if self._is_noter_main(heading):
                main_sections['notes_collection'].append(heading)
                seen_noter_main = True
                print(f"      üìö Noter main section: '{heading}'")
                # Don't set in_notes_subsection yet - wait for actual subsections
                continue

            # Layer 3: Semantic detection (after seeing "Noter" main section)
            if seen_noter_main and not self._is_end_marker(heading):
                if self._contains_note_keywords(heading):
                    in_notes_subsection = True
                    note_headings.append(heading)
                    print(f"      üìù Note (semantic): '{heading[:50]}...'")
                    continue

            # Layer 4: Stop at end markers
            if self._is_end_marker(heading):
                print(f"      üõë End marker: '{heading[:40]}...'")
                break

            # Route remaining sections (fallback routing)
            if not in_notes_subsection:
                # OPTION A already handled above (moved to Priority 0)

                # OPTION B: Try fuzzy matching with dictionary for unrouted sections
                routed = False
                match = self.dictionary.match_term(heading, fuzzy_threshold=0.70)
                if match and match.confidence >= 0.70:
                    # Map dictionary categories to agent IDs
                    category_to_agent = {
                        'balance_sheet': 'financial_agent',
                        'income_statement': 'financial_agent',
                        'cash_flow': 'financial_agent',
                        'notes': 'notes_collection',
                        'governance': 'governance_agent',
                        'board': 'governance_agent',
                        'audit': 'governance_agent',
                        'management_report': 'governance_agent',
                        'property': 'property_agent',
                        'operations': 'operations_agent'
                    }

                    agent_id = category_to_agent.get(match.category)
                    if agent_id and agent_id in main_sections:
                        main_sections[agent_id].append(heading)
                        routed = True

                # OPTION C: Track unrouted sections for LLM classification
                if not routed:
                    unrouted_sections.append(heading)

        # Route note subsections using NoteSemanticRouter
        if note_headings:
            note_sections = self.note_router.route_headings(note_headings)

        # OPTION C: LLM classification fallback for remaining unrouted sections
        if unrouted_sections:
            print(f"   ü§ñ LLM fallback: Classifying {len(unrouted_sections)} unrouted sections...")
            llm_routes = self._classify_sections_llm(unrouted_sections)

            # V3.0: Handle multi-agent routing (one heading ‚Üí multiple agents)
            for heading, classification in llm_routes.items():
                # Extract agent list (supports both single-agent and multi-agent)
                agents_list = classification.get('agents', [classification.get('primary_agent')])

                # Route heading to ALL applicable agents
                for agent_id in agents_list:
                    if agent_id in main_sections:
                        main_sections[agent_id].append(heading)
                        print(f"      üéØ LLM routed: '{heading[:40]}...' ‚Üí {agent_id} (conf: {classification.get('confidence', 0):.2f})")

        elapsed = time.time() - start_time

        routing = SectionRouting(
            main_sections=main_sections,
            note_sections=note_sections,
            routing_time=elapsed
        )

        # Print routing summary
        total_main = sum(len(headings) for headings in main_sections.values())
        total_notes = sum(len(headings) for headings in note_sections.values())
        print(f"   ‚úÖ Routing: {total_main} main sections, {total_notes} note sections ({elapsed:.2f}s)")

        return routing

    def _get_pdf_page_count(self, pdf_path: str) -> int:
        """Get total page count from PDF"""
        import fitz
        doc = fitz.open(pdf_path)
        count = doc.page_count
        doc.close()
        return count

    def _find_heading_in_pdf(self, pdf_path: str, heading: str) -> Optional[int]:
        """Search PDF text for heading, return page number (0-indexed)"""
        import fitz
        doc = fitz.open(pdf_path)
        heading_lower = heading.lower()

        for page_num in range(doc.page_count):
            page = doc[page_num]
            text = page.get_text().lower()
            if heading_lower in text:
                doc.close()
                return page_num

        doc.close()
        return None

    def _get_governance_pages_for_scanned_pdf(self, pdf_path: str) -> List[int]:
        """
        Get governance agent pages for cross-agent fallback on scanned PDFs.

        For scanned PDFs, governance sections (f√∂rvaltningsber√§ttelse) typically
        include or precede financial statements. This method extracts the page
        allocation that governance_agent would receive.

        Returns:
            List of page numbers (0-indexed) allocated to governance agent
        """
        if not hasattr(self, 'structure_cache') or not self.structure_cache:
            return []

        # Find governance section headings from structure
        governance_headings = []
        for section in self.structure_cache.sections:
            heading = section['heading']
            heading_normalized = self._normalize_swedish(heading)

            # Check if this is a governance section
            for keyword in self.main_section_keywords.get('governance_agent', []):
                keyword_normalized = self._normalize_swedish(keyword)
                if keyword_normalized in heading_normalized:
                    governance_headings.append(heading)
                    break

        if not governance_headings:
            return []

        # Get pages using same allocation logic
        # Note: This will use Methods 1-2 (provenance + section-range)
        # but NOT Method 3 (keyword search, which fails on scanned PDFs)
        pages = self._get_pages_for_sections(
            pdf_path,
            governance_headings,
            fallback_pages=5,
            agent_id='governance_agent'
        )

        return pages

    def _get_pages_for_sections(
        self,
        pdf_path: str,
        section_headings: List[str],
        fallback_pages: int = 5,
        agent_id: str = None
    ) -> List[int]:
        """
        P0-2 & P0-3: ADAPTIVE PAGE ALLOCATION STRATEGY

        CRITICAL FIX: Collect pages from ALL section headings, not just first!

        Strategy:
        1. Provenance pages from ALL section headings (not just first)
        2. Add context pages around EACH heading
        3. Document-size-aware allocation (aggressive for small docs)
        4. Keyword-based detection (backup)
        5. Rank by relevance and limit to max pages (cost control)
        """
        pages = []
        total_pages = self._get_pdf_page_count(pdf_path)

        # Method 1: Provenance pages from ALL section headings (CRITICAL FIX)
        if hasattr(self, 'structure_cache') and self.structure_cache:
            for heading in section_headings:
                for section in self.structure_cache.sections:
                    if section['heading'] == heading and section.get('page') is not None:
                        page = section['page']
                        pages.append(page)

                        # Add context pages around THIS heading
                        if agent_id in ['financial_agent', 'revenue_breakdown_agent', 'operating_costs_agent']:
                            # PHASE 4B: Balanced context pages (header + 3 pages = 4 per section)
                            # All financial agents get same treatment (income statement on same pages)
                            pages.extend([page+1, page+2, page+3])
                        elif agent_id == 'property_agent':
                            # Property: Header + next 3 pages
                            pages.extend([page+1, page+2, page+3])
                        elif agent_id == 'governance_agent':
                            # Governance: Header + next 3 pages
                            pages.extend([page+1, page+2, page+3])
                        elif agent_id and 'notes_' in agent_id:
                            # Notes: Header + next 2 pages (increased coverage)
                            pages.extend([page+1, page+2])

                        break  # Found this heading, move to next

        # Method 2: Section-range estimation using Docling structure boundaries
        # PHASE 4B ADAPTIVE: Use next section's location to estimate current section's extent
        # NO HARDCODED PAGE NUMBERS - fully adaptive to document structure
        if hasattr(self, 'structure_cache') and self.structure_cache and self.structure_cache.sections:
            for heading in section_headings:
                # Find this section's index in Docling structure
                section_idx = None
                for i, section in enumerate(self.structure_cache.sections):
                    if section['heading'] == heading:
                        section_idx = i
                        break

                if section_idx is not None and self.structure_cache.sections[section_idx].get('page') is not None:
                    start_page = self.structure_cache.sections[section_idx]['page']

                    # Estimate end by finding next section boundary
                    if section_idx + 1 < len(self.structure_cache.sections):
                        next_section = self.structure_cache.sections[section_idx + 1]
                        next_page = next_section.get('page')
                        if next_page is not None:
                            # Section spans from start to next section
                            end_page = next_page
                        else:
                            # Next section missing page: use agent-specific max span
                            max_span = {'financial_agent': 10, 'property_agent': 6, 'governance_agent': 5}.get(agent_id, 5)
                            end_page = start_page + max_span
                    else:
                        # Last section: use agent-specific max span (avoid signature pages)
                        max_span = {'financial_agent': 10, 'property_agent': 6, 'governance_agent': 5}.get(agent_id, 5)
                        end_page = min(start_page + max_span, total_pages - 2)

                    # Add section range (bounded by document length)
                    section_range = range(start_page, min(end_page, total_pages))
                    pages.extend(section_range)
                    print(f"      üìä Section '{heading[:30]}...' range: pages {start_page+1}-{min(end_page, total_pages)} ({len(section_range)} pages)")

        # Method 3: Keyword-based detection (machine-readable PDFs only)
        # CRITICAL: This finds actual content pages, not just where Docling detected section headers
        # NOTE: Only works for machine-readable PDFs (scanned PDFs have 0 chars/page)
        if agent_id in ['financial_agent', 'revenue_breakdown_agent', 'operating_costs_agent', 'property_agent', 'operations_agent']:
            if hasattr(self, 'topology') and self.topology.classification == "machine_readable":
                keyword_pages = self._find_pages_by_content_keywords(pdf_path, agent_id)
                if keyword_pages:
                    pages.extend(keyword_pages)
                    print(f"      üîç Keyword search found: {len(keyword_pages)} pages")

        # Deduplicate FIRST (before checking fallback conditions)
        pages = sorted(set(pages))

        # Method 4: Cross-agent page sharing (for scanned PDFs where keyword search doesn't work)
        # CRITICAL FIX: For scanned PDFs, financial agents should use governance pages as fallback
        # Governance pages (f√∂rvaltningsber√§ttelse) typically include or precede financial statements
        # IMPORTANT: Check AFTER deduplication to get accurate count
        if agent_id in ['financial_agent', 'revenue_breakdown_agent', 'operating_costs_agent']:
            if hasattr(self, 'topology') and self.topology.classification == "scanned":
                # Check if we have adequate pages already (after deduplication)
                if len(pages) < 8:
                    # Get governance agent pages from structure cache
                    governance_pages = self._get_governance_pages_for_scanned_pdf(pdf_path)
                    if governance_pages and len(governance_pages) >= 8:
                        # Use middle section of governance pages (likely where financial content is)
                        # Typically pages 6-14 in a 12-page governance allocation
                        start_idx = len(governance_pages) // 3  # Skip intro pages
                        end_idx = min(start_idx + 8, len(governance_pages))
                        fallback_pages = governance_pages[start_idx:end_idx]
                        pages.extend(fallback_pages)
                        # Re-deduplicate after adding governance pages
                        pages = sorted(set(pages))
                        print(f"      üîÑ Scanned PDF fallback: +{len(fallback_pages)} governance pages")
                    else:
                        # Last resort: use typical financial statement pages
                        typical_fin_pages = list(range(6, min(15, total_pages)))
                        pages.extend(typical_fin_pages)
                        pages = sorted(set(pages))
                        print(f"      üîÑ Scanned PDF fallback: Using typical pages {[p+1 for p in typical_fin_pages]}")

        # Method 5: Fallback to first N pages (last resort)
        if not pages:
            pages = list(range(min(fallback_pages, total_pages)))

        # DEBUG: Show page allocation strategy
        if agent_id:
            print(f"      üìÑ {agent_id}: {len(pages)} pages allocated (before limit)")

        # Optimization: Limit to reasonable max (cost control)
        max_pages = {
            'financial_agent': 20,
            'governance_agent': 12,
            'property_agent': 10,
            'operations_agent': 8
        }.get(agent_id, 10)

        if len(pages) > max_pages:
            # Keep first max_pages (already sorted, provenance pages come first)
            pages = pages[:max_pages]
            print(f"      ‚úÇÔ∏è  Limited to {max_pages} pages (cost control)")

        return pages

    def _find_pages_by_content_keywords(self, pdf_path: str, agent_id: str) -> List[int]:
        """
        Find pages containing agent-specific content keywords.
        Returns pages most likely to contain relevant data.
        """
        import fitz  # PyMuPDF - import locally to avoid top-level dependency

        # Agent-specific content keywords (Swedish BRF terminology)
        AGENT_KEYWORDS = {
            'financial_agent': [
                'resultatr√§kning', 'balansr√§kning', 'kassafl', 'int√§kter', 'kostnader',
                'tillg√•ngar', 'skulder', 'eget kapital', 'tkr', 'resultat', '√∂verskott'
            ],
            'property_agent': [
                'fastighetsbeteckning', 'adress', 'bygg√•r', 'byggnads√•r', 'l√§genheter',
                'antal l√§genheter', 'energiklass', 'energideklaration', 'kommun'
            ],
            'governance_agent': [
                'styrelse', 'ordf√∂rande', 'ledamot', 'revisor', 'valberedning',
                'styrelsem√∂te', 'f√∂rvaltningsber√§ttelse'
            ],
            'operations_agent': [
                'drift', 'underh√•ll', 'reparation', 'st√§dning', 'sn√∂r√∂jning',
                'fastighetssk√∂tsel', 'energi', 'v√§rme', 'vatten', 'el'
            ],
            'notes_accounting_agent': [
                'redovisningsprinciper', 'v√§rdering', 'avskrivning', 'not 1', 'not 2'
            ],
            'notes_loans_agent': [
                'fastighetsl√•n', 'l√•ngfristiga skulder', 'r√§nta', 'amortering', 'not'
            ],
            'notes_buildings_agent': [
                'byggnader', 'mark', 'fastighetsv√§rde', 'avskrivning', 'not'
            ],
            'notes_receivables_agent': [
                'fordringar', 'kundfordringar', '√∂vriga fordringar', 'not'
            ],
            'notes_reserves_agent': [
                'fond', 'yttre underh√•ll', 'reserv', 'avs√§ttning', 'not'
            ],
            'notes_tax_agent': [
                'skatt', 'inkomstskatt', 'uppskjuten skatt', 'fastighetsskatt', 'not'
            ],
            'notes_other_agent': [
                'not', 'noter', 'anm√§rkning', 'till√§gg'
            ]
        }

        keywords = AGENT_KEYWORDS.get(agent_id, [])
        if not keywords:
            return []

        # Search all pages for keywords
        try:
            doc = fitz.open(pdf_path)
            page_scores = {}

            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text().lower()

                # Count keyword matches
                score = sum(1 for keyword in keywords if keyword in text)
                if score > 0:
                    page_scores[page_num] = score

            doc.close()

            # Return top 5 pages by score
            if page_scores:
                sorted_pages = sorted(page_scores.items(), key=lambda x: x[1], reverse=True)
                return [page_num for page_num, score in sorted_pages[:5]]

        except Exception as e:
            print(f"   ‚ö†Ô∏è Content keyword search failed: {e}")

        return []

    # _render_pdf_pages() is now inherited from BaseExtractor

    def _build_selective_context(
        self,
        agent_id: str,
        pass1_results: Dict,
        pass2_results: Dict
    ) -> str:
        """Build selective context (only relevant fields to save tokens)"""
        context_fields = []

        # Governance context (useful for all agents)
        if 'governance_agent' in pass1_results:
            gov = pass1_results['governance_agent']
            if gov.get('chairman'):
                context_fields.append(f"Chairman: {gov['chairman']}")

        # Property context
        if 'property_agent' in pass1_results:
            prop = pass1_results['property_agent']
            if prop.get('address'):
                context_fields.append(f"Property: {prop['address']}")
            if prop.get('apartments'):
                context_fields.append(f"Apartments: {prop['apartments']}")

        # Financial context (for note agents)
        if agent_id.startswith('notes_') and 'financial_agent' in pass2_results:
            fin = pass2_results['financial_agent']
            if fin.get('assets'):
                context_fields.append(f"Assets: {fin['assets']}")
            if fin.get('liabilities'):
                context_fields.append(f"Liabilities: {fin['liabilities']}")

        if not context_fields:
            return ""

        return "\n\nContext from previous extraction:\n" + "\n".join(context_fields)

    # _parse_json_with_fallback() is now inherited from BaseExtractor

    # _extract_agent() is now inherited from BaseExtractor
    # Note: BaseExtractor includes all agent prompts, extraction logic, retry handling,
    # and evidence tracking. The optimal pipeline uses the shared implementation.

    def extract_pass1(
        self,
        routing: SectionRouting
    ) -> Dict[str, Dict]:
        """
        Pass 1: Extract high-level fields (governance, property) in parallel.

        Args:
            routing: Section routing results from Stage 3

        Returns:
            Dict mapping agent_id -> extraction results
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed

        start_time = time.time()

        # Pass 1 agents (main sections, simple extraction)
        pass1_agents = [
            'governance_agent',
            'property_agent'
        ]

        results = {}

        # Parallel execution
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_agent = {}

            for agent_id in pass1_agents:
                section_headings = routing.main_sections.get(agent_id, [])
                if section_headings or agent_id == 'governance_agent':  # Always run governance
                    future = executor.submit(
                        self._extract_agent,
                        self.pdf_path_cache,  # Will need to pass this
                        agent_id,
                        section_headings
                    )
                    future_to_agent[future] = agent_id

            # Collect results
            for future in as_completed(future_to_agent):
                agent_id = future_to_agent[future]
                try:
                    results[agent_id] = future.result(timeout=60)
                except Exception as e:
                    results[agent_id] = {
                        "agent_id": agent_id,
                        "status": "error",
                        "error": str(e)
                    }

        elapsed = time.time() - start_time
        print(f"   ‚úÖ Pass 1: {len(results)} agents completed ({elapsed:.1f}s)")

        return results

    def extract_pass2(
        self,
        routing: SectionRouting,
        pass1_results: Dict[str, Dict]
    ) -> Dict[str, Dict]:
        """
        Pass 2: Extract financial + notes sequentially with hierarchical context.

        Args:
            routing: Section routing results
            pass1_results: Results from Pass 1

        Returns:
            Dict mapping agent_id -> extraction results
        """
        start_time = time.time()

        results = {}

        # Extract financial agent first
        financial_headings = routing.main_sections.get('financial_agent', [])
        if financial_headings:
            results['financial_agent'] = self._extract_agent(
                self.pdf_path_cache,
                'financial_agent',
                financial_headings,
                context=pass1_results
            )

        # Sprint 1+2: Extract revenue_breakdown_agent (uses same headings as financial_agent)
        # Revenue breakdown targets income statement (Resultatr√§kning) pages 6-8
        if financial_headings:
            results['revenue_breakdown_agent'] = self._extract_agent(
                self.pdf_path_cache,
                'revenue_breakdown_agent',
                financial_headings,  # Same sections (Resultatr√§kning contains revenue)
                context=pass1_results
            )

        # Sprint 1+2 Day 3: Extract operating_costs_agent (uses same headings as financial_agent)
        # Operating costs breakdown targets income statement (Resultatr√§kning) pages 6-8
        if financial_headings:
            results['operating_costs_agent'] = self._extract_agent(
                self.pdf_path_cache,
                'operating_costs_agent',
                financial_headings,  # Same sections (Resultatr√§kning contains operating costs)
                context=pass1_results
            )

        # P0 FIX: Extract operations_agent (maintenance, energy, contracts)
        # Operations content typically in f√∂rvaltningsber√§ttelse or dedicated sections
        operations_headings = routing.main_sections.get('operations_agent', [])
        if operations_headings:
            results['operations_agent'] = self._extract_agent(
                self.pdf_path_cache,
                'operations_agent',
                operations_headings,
                context=pass1_results
            )

        # Extract note agents sequentially
        for agent_id, section_headings in routing.note_sections.items():
            # Build hierarchical context (Pass 1 + financial + previous notes)
            context = {
                **pass1_results,
                'financial': results.get('financial_agent', {}),
                'previous_notes': {k: v for k, v in results.items() if k.startswith('notes_')}
            }

            results[agent_id] = self._extract_agent(
                self.pdf_path_cache,
                agent_id,
                section_headings,
                context=context
            )

        # P1-NOTES: Comprehensive extraction if Docling missed notes
        # If we detected "Noter" section but <5 individual notes, scan entire Noter range
        if routing.main_sections.get('notes_collection') and len(routing.note_sections) < 5:
            print(f"   ‚ö†Ô∏è  Only {len(routing.note_sections)} notes detected by Docling")
            print(f"   üîç Running comprehensive notes extraction (Noter section)...")

            # Find Noter page range dynamically
            noter_start_page = None
            end_page = self._get_pdf_page_count(self.pdf_path_cache)

            # Find where "Noter" section starts
            if hasattr(self, 'structure_cache') and self.structure_cache:
                for section in self.structure_cache.sections:
                    if self._is_noter_main(section['heading']):
                        noter_start_page = section.get('page')
                        break

            # If found, scan from Noter to end of document (or signature page)
            if noter_start_page is not None:
                # Scan Noter section (typically 5-8 pages)
                noter_pages = list(range(noter_start_page, min(noter_start_page + 8, end_page - 2)))
                print(f"       üìÑ Noter pages: {[p+1 for p in noter_pages]} (0-indexed: {noter_start_page} to {noter_start_page+7})")

                # Override page allocation by manually calling extraction with specific pages
                # We'll temporarily store pages and call _extract_agent
                saved_structure = self.structure_cache

                # Call extraction with manual override by creating temp section
                import fitz
                doc = fitz.open(self.pdf_path_cache)

                # Render Noter pages
                images = []
                page_labels = []
                for page_num in noter_pages[:12]:  # Max 12 pages
                    if page_num < len(doc):
                        page = doc[page_num]
                        zoom = 200 / 72
                        mat = fitz.Matrix(zoom, zoom)
                        pix = page.get_pixmap(matrix=mat)
                        img_bytes = pix.tobytes("png")
                        images.append(img_bytes)
                        page_labels.append(f"Page {page_num + 1}")

                doc.close()

                # Build prompt for comprehensive notes
                prompt = self.AGENT_PROMPTS['comprehensive_notes_agent']
                prompt += f"\n\nScanning pages {[p+1 for p in noter_pages]} for all notes content.\n"

                # Call OpenAI directly with Noter pages
                import base64
                content = [{"type": "text", "text": prompt}]
                for img_bytes, label in zip(images, page_labels):
                    img_b64 = base64.b64encode(img_bytes).decode('utf-8')
                    content.append({"type": "text", "text": f"\n--- {label} ---"})
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{img_b64}",
                            "detail": "high"
                        }
                    })

                try:
                    from openai import OpenAI
                    import os
                    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

                    response = client.chat.completions.create(
                        model="gpt-4o-2024-11-20",
                        messages=[{"role": "user", "content": content}],
                        max_tokens=4000,  # More tokens for comprehensive extraction
                        temperature=0
                    )

                    raw_content = response.choices[0].message.content
                    extracted_data = self._parse_json_with_fallback(raw_content)

                    if extracted_data:
                        results['comprehensive_notes_agent'] = {
                            "agent_id": "comprehensive_notes_agent",
                            "status": "success",
                            "data": extracted_data,
                            "section_headings": ["Noter (complete section)"],
                            "pages_rendered": [p+1 for p in noter_pages[:12]],
                            "num_images": len(images),
                            "evidence_pages": extracted_data.get('evidence_pages', []),
                            "extraction_time": 0,
                            "model": "gpt-4o-2024-11-20",
                            "tokens_used": response.usage.total_tokens if hasattr(response, 'usage') else 0
                        }
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Comprehensive notes extraction failed: {e}")

        elapsed = time.time() - start_time
        print(f"   ‚úÖ Pass 2: {len(results)} agents completed ({elapsed:.1f}s)")

        return results

    def validate_extraction(
        self,
        pass1_results: Dict[str, Dict],
        pass2_results: Dict[str, Dict]
    ) -> Dict[str, Any]:
        """
        Pass 3: Validate extraction quality against 95/95 gates.

        Quality gates:
        1. Coverage: ‚â•95% of required fields extracted
        2. Numeric QC: Financial fields within tolerance
        3. Evidence: ‚â•95% of extractions cite source pages

        Args:
            pass1_results: Results from Pass 1
            pass2_results: Results from Pass 2

        Returns:
            Quality metrics dict
        """
        start_time = time.time()

        all_results = {**pass1_results, **pass2_results}

        # Gate 1: Coverage check
        total_agents = len(all_results)
        successful_agents = sum(1 for r in all_results.values() if r.get('status') != 'error')
        coverage = successful_agents / total_agents if total_agents > 0 else 0.0

        # Gate 2: Numeric QC (placeholder - would use Gracian qc.py)
        numeric_qc_pass = True  # TODO: Implement actual numeric validation

        # Gate 3: Evidence tracking
        agents_with_evidence = sum(
            1 for r in all_results.values()
            if r.get('evidence_pages') and len(r.get('evidence_pages', [])) > 0
        )
        evidence_ratio = agents_with_evidence / total_agents if total_agents > 0 else 0.0

        # Overall quality score
        overall_score = (coverage + evidence_ratio) / 2

        quality = {
            "coverage": coverage,
            "numeric_qc_pass": numeric_qc_pass,
            "evidence_ratio": evidence_ratio,
            "overall_score": overall_score,
            "needs_coaching": overall_score < 0.95,
            "validation_time": time.time() - start_time
        }

        # Print quality report
        print(f"   Coverage: {coverage:.1%} ({successful_agents}/{total_agents} agents)")
        print(f"   Numeric QC: {'‚úÖ Pass' if numeric_qc_pass else '‚ùå Fail'}")
        print(f"   Evidence: {evidence_ratio:.1%} ({agents_with_evidence}/{total_agents} agents)")
        print(f"   Overall: {overall_score:.1%} {'‚úÖ' if overall_score >= 0.95 else '‚ö†Ô∏è'}")

        return quality

    def extract_document(
        self,
        pdf_path: str
    ) -> ExtractionResult:
        """
        Main extraction pipeline for a single document.

        Full 5-stage process:
        1. Topology detection
        2. Structure detection
        3. Section routing
        4. Hierarchical extraction (3 passes)
        5. Quality validation
        """
        overall_start = time.time()
        print(f"\n{'='*70}")
        print(f"OPTIMAL BRF PIPELINE - {Path(pdf_path).name}")
        print(f"{'='*70}\n")

        # STAGE 1: Topology detection
        print("üìä STAGE 1: PDF Topology Detection")
        topology = self.analyze_topology(pdf_path)

        # Store topology for access by _get_pages_for_sections() (Phase 2E)
        self.topology = topology

        # STAGE 2: Structure detection
        print("\nüîç STAGE 2: Structure Detection (Docling)")
        structure = self.detect_structure(pdf_path, topology)

        # STAGE 3: Section routing
        print("\nüß≠ STAGE 3: Section Routing (Hybrid)")
        routing = self.route_sections(structure)

        # Cache PDF path for extraction methods
        self.pdf_path_cache = pdf_path

        # STAGE 4: Hierarchical extraction
        print("\nüéØ STAGE 4: Hierarchical Extraction")

        # Pass 1: High-level extraction (governance, property) - parallel
        print("   üîÑ Pass 1: High-level (governance, property)...")
        pass1_result = self.extract_pass1(routing)

        # Pass 2: Financial + notes - sequential
        print("   üîÑ Pass 2: Financial + notes (detailed)...")
        pass2_result = self.extract_pass2(routing, pass1_result)

        # No Pass 3 coaching in Phase 2 (add later if needed)
        pass3_result = {}

        # STAGE 5: Quality validation
        print("\n‚úÖ STAGE 5: Quality Validation")
        quality_metrics = self.validate_extraction(pass1_result, pass2_result)

        total_time = time.time() - overall_start
        total_cost = 0.0  # Placeholder

        result = ExtractionResult(
            pdf_path=pdf_path,
            topology=topology,
            structure=structure,
            routing=routing,
            pass1_result=pass1_result,
            pass2_result=pass2_result,
            pass3_result=pass3_result,
            brf_report=None,  # TODO: Build Pydantic model
            quality_metrics=quality_metrics,
            total_time=total_time,
            total_cost=total_cost
        )

        # Save results
        output_file = self.output_dir / f"{Path(pdf_path).stem}_optimal_result.json"

        # Combine all agent results
        agent_results = {}
        agent_results.update(pass1_result)
        agent_results.update(pass2_result)
        agent_results.update(pass3_result)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'pdf': pdf_path,
                'topology': asdict(topology),
                'structure': {
                    'num_sections': structure.num_sections,
                    'method': structure.method,
                    'cached': structure.cached,
                    'extraction_time': structure.extraction_time
                },
                'routing': {
                    'main_sections': {k: len(v) for k, v in routing.main_sections.items()},
                    'note_sections': {k: len(v) for k, v in routing.note_sections.items()},
                    'routing_time': routing.routing_time
                },
                'agent_results': agent_results,  # ADD: Actual extraction data
                'quality_metrics': quality_metrics,
                'total_time': total_time,
                'total_cost': total_cost
            }, f, indent=2, ensure_ascii=False)

        print(f"\n{'='*70}")
        print(f"‚úÖ PIPELINE COMPLETE")
        print(f"   Total time: {total_time:.1f}s")
        print(f"   Total cost: ${total_cost:.3f}")
        print(f"   Output: {output_file}")
        print(f"{'='*70}\n")

        return result

    def close(self):
        """Clean up resources"""
        if self.note_router:
            self.note_router.close()
        print("‚úÖ Pipeline closed")


def main():
    """Test optimal pipeline on sample document"""
    import sys
    from dotenv import load_dotenv

    # Load environment variables from Gracian Pipeline root
    gracian_root = Path(__file__).resolve().parent.parent.parent.parent
    env_path = gracian_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"‚úÖ Loaded .env from {env_path}")
    else:
        print(f"‚ö†Ô∏è  Warning: .env file not found at {env_path}")

    if len(sys.argv) < 2:
        print("Usage: python optimal_brf_pipeline.py <pdf_path>")
        sys.exit(1)

    pdf_path = sys.argv[1]

    if not os.path.exists(pdf_path):
        print(f"Error: PDF not found: {pdf_path}")
        sys.exit(1)

    # Run pipeline
    pipeline = OptimalBRFPipeline(
        cache_dir="results/cache",
        output_dir="results/optimal_pipeline",
        enable_caching=True
    )

    try:
        result = pipeline.extract_document(pdf_path)
        print("\n‚úÖ Extraction successful!")
        print(f"   Sections detected: {result.structure.num_sections}")
        print(f"   Main section agents: {sum(len(v) for v in result.routing.main_sections.values())}")
        print(f"   Note section agents: {sum(len(v) for v in result.routing.note_sections.values())}")

    finally:
        pipeline.close()


if __name__ == "__main__":
    main()
