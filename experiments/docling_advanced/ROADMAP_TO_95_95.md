# üéØ Roadmap to 95/95: Production-Scale BRF Extraction

**Date**: October 14, 2025
**Current Status**: Phase 4B Complete - **84% average coverage** (7 PDFs validated)
**Target**: **95% coverage, 95% accuracy** on 27,000 PDF corpus
**Gap**: **11 percentage points** to target

---

## üìä Current Achievement Analysis

### ‚úÖ What's Working (Phase 4B Validated)

1. **Cross-Agent Fallback for Financial Extraction** ‚úÖ
   - **Impact**: 36.7% ‚Üí 100% on scanned PDFs (brf_81563)
   - **Strategy**: Financial agents fall back to governance pages when keyword search fails
   - **Coverage**: 83.8% average on scanned PDFs

2. **Adaptive Page Allocation** ‚úÖ
   - **Impact**: 4 ‚Üí 12 pages per agent (3x increase)
   - **Strategy**: Collect from ALL section headings + document-size-aware limits
   - **Result**: Eliminated hardcoded page bottlenecks

3. **Comprehensive Notes Extraction** ‚úÖ
   - **Impact**: Works around Docling detection limitations
   - **Strategy**: Single agent extracts entire Noter section (pages 11-16)
   - **Coverage**: Catches missed notes reliably

4. **3-Layer Routing Fallback** ‚úÖ
   - **Impact**: 50% ‚Üí 94.3% match rate
   - **Strategy**: Swedish normalization ‚Üí Fuzzy matching ‚Üí LLM classification
   - **Quality**: Robust against heading variations

### ‚ùå What's Missing (Gap to 95%)

1. **Property/Operations Sections**: **~0% coverage** in many PDFs
   - **Root Cause**: Docling detects 0 property/operations sections
   - **Hypothesis**: Content embedded in f√∂rvaltningsber√§ttelse (governance)
   - **Expected Impact**: **+5-8 percentage points**

2. **Some Notes Still Missing**: Only 3/20+ notes detected in some PDFs
   - **Root Cause**: Docling note detection incomplete
   - **Current Mitigation**: Comprehensive notes agent (partial)
   - **Expected Impact**: **+2-4 percentage points**

3. **Low Performer Outliers**: brf_268882 at 71.7%
   - **Root Cause**: Unknown (needs analysis)
   - **Expected Impact**: **+2-3 percentage points** (bring outliers up)

4. **Untested PDFs**: 3/10 Phase 4 PDFs didn't complete (test harness bug)
   - **Status**: Pipeline works, test script had JSON serialization error
   - **Next Step**: Complete remaining 3 PDFs

---

## üöÄ Phase 5: Property/Operations Cross-Extraction

**Timeline**: 2-3 hours
**Expected Impact**: **+5-8 percentage points** (84% ‚Üí 89-92%)
**Priority**: **P0 - CRITICAL**

### Strategy

**Problem**: Property and operations content exists in PDFs, but Docling doesn't detect dedicated sections for them.

**Solution**: Extend cross-agent fallback (proven successful for financial agents) to property and operations agents.

**Hypothesis**: Like financial statements embedded in f√∂rvaltningsber√§ttelse, property and operations details are also in governance pages.

### Implementation

```python
# In _get_pages_for_sections() method (optimal_brf_pipeline.py)

# After existing financial cross-agent fallback (lines 876-901):

# Method 4B: Property/Operations cross-agent fallback
if agent_id in ['property_agent', 'operations_agent']:
    if hasattr(self, 'topology') and self.topology.classification == "scanned":
        # Check if we have adequate pages already (after deduplication)
        if len(pages) < 8:
            # Get governance agent pages from structure cache
            governance_pages = self._get_governance_pages_for_scanned_pdf(pdf_path)
            if governance_pages and len(governance_pages) >= 8:
                # For property: use beginning section (typically pages 2-6)
                # For operations: use middle section (typically pages 4-10)
                if agent_id == 'property_agent':
                    start_idx = 0  # Start from beginning
                    end_idx = min(8, len(governance_pages))
                else:  # operations_agent
                    start_idx = len(governance_pages) // 4  # Skip intro
                    end_idx = min(start_idx + 8, len(governance_pages))

                fallback_pages = governance_pages[start_idx:end_idx]
                pages.extend(fallback_pages)
                pages = sorted(set(pages))  # Re-deduplicate
                print(f"      üîÑ {agent_id} fallback: +{len(fallback_pages)} governance pages")
```

### Validation

**Test on**:
- brf_268882 (property: 0/6, operations: 0/5)
- brf_53546 (property: 2/6, operations: 1/5)
- brf_47903 (property: 1/6, operations: 0/5)

**Success Criteria**:
- Property coverage: 0% ‚Üí 60%+ (3-4/6 fields)
- Operations coverage: 0% ‚Üí 40%+ (2-3/5 fields)
- No regression on other agents

---

## üöÄ Phase 6: Enhanced Note Detection

**Timeline**: 3-4 hours
**Expected Impact**: **+2-4 percentage points** (89-92% ‚Üí 91-96%)
**Priority**: **P1 - HIGH**

### Strategy

**Problem**: Docling detects only 3 notes in brf_268882, but there are likely 20+ notes.

**Current Mitigation**: Comprehensive notes agent extracts pages 11-16, but:
- Doesn't know exact note boundaries
- May miss notes outside pages 11-16
- Can't distinguish between notes vs other content

**Solution**: Two-phase approach:
1. **Phase 6A**: LLM-based note boundary detection
2. **Phase 6B**: Individual note extraction with context

### Implementation

#### Phase 6A: Note Boundary Detection

```python
def _detect_note_boundaries_with_llm(self, pdf_path: str, note_pages: List[int]) -> List[Dict]:
    """
    Use LLM to detect individual note boundaries within Noter section.

    Returns:
        List of {note_number, title, start_page, end_page}
    """
    # Render note pages to images
    images = self._render_pages_to_base64(pdf_path, note_pages)

    prompt = f"""
    This is the NOTER (Notes) section of a Swedish BRF annual report.

    Please identify ALL individual notes in this section.

    For each note, extract:
    - note_number (e.g., "NOT 1", "NOT 2", etc.)
    - title (the note heading)
    - start_page (which page the note starts on, 0-indexed)
    - end_page (which page the note ends on, 0-indexed)

    Return as JSON array: [{{"note_number": "NOT 1", "title": "...", "start_page": 10, "end_page": 10}}, ...]
    """

    # Call GPT-4o with vision
    response = self._call_openai_vision(images, prompt)

    return json.loads(response)
```

#### Phase 6B: Individual Note Extraction

```python
def _extract_individual_notes(self, pdf_path: str, note_boundaries: List[Dict]) -> Dict:
    """
    Extract each note individually with its specific context.
    """
    notes_data = {}

    for note in note_boundaries:
        note_num = note['note_number']
        note_title = note['title']
        pages = list(range(note['start_page'], note['end_page'] + 1))

        # Route to appropriate notes agent based on title
        agent_id = self._route_note_to_agent(note_title)

        # Extract with focused context
        result = self._extract_with_agent(
            agent_id=agent_id,
            pdf_path=pdf_path,
            pages=pages,
            context=f"Note {note_num}: {note_title}"
        )

        notes_data[f"note_{note_num}"] = result

    return notes_data
```

### Validation

**Test on**:
- brf_268882 (only 3 notes detected, expect 20+)
- brf_53546 (complete note extraction)
- brf_47903 (verify no regression)

**Success Criteria**:
- Note detection: 3 ‚Üí 15-20 notes per PDF
- Note coverage: 60% ‚Üí 85%+ (12-15/18 note fields)
- No significant cost increase (use caching)

---

## üöÄ Phase 7: Outlier Analysis & Fixes

**Timeline**: 2-3 hours
**Expected Impact**: **+2-3 percentage points** (91-96% ‚Üí 93-99%)
**Priority**: **P1 - HIGH**

### Strategy

**Problem**: Some PDFs still have unexpectedly low coverage:
- brf_268882: 71.7% (outlier)
- 3 untested PDFs: brf_276796, brf_48663, brf_83301

**Solution**: Systematic analysis and targeted fixes.

### Analysis Tasks

1. **Deep Dive on brf_268882**:
   ```bash
   # Compare with ground truth (if available)
   # Analyze which specific fields are missing
   # Check if it's a document structure issue or extraction issue
   ```

2. **Complete Testing on Remaining 3 PDFs**:
   ```bash
   # Fix test harness JSON serialization (already done)
   # Run on brf_276796, brf_48663, brf_83301
   # Analyze any new failure modes
   ```

3. **Pattern Detection**:
   - Are low performers all scanned PDFs?
   - Are low performers all from specific regions?
   - Do low performers have unusual section structures?

### Targeted Fixes

Based on analysis, implement:
- **Additional fallback strategies** for edge cases
- **Better section boundary detection** for unusual layouts
- **Improved context windows** for sparse documents

---

## üöÄ Phase 8: Scale Testing (100 PDFs)

**Timeline**: 1 day
**Expected Impact**: Validation, not improvement
**Priority**: **P0 - CRITICAL** (before production)

### Strategy

**Goal**: Validate that 95/95 performance holds on larger, more diverse sample.

### Test Set Selection

```python
# 100 PDFs balanced across:
# - 50 Hjorthagen, 50 SRS (regional diversity)
# - 50 scanned, 50 machine-readable (topology diversity)
# - 50 <20 pages, 50 >20 pages (size diversity)
# - 50 2020-2022, 50 2023-2024 (temporal diversity)
```

### Metrics to Track

1. **Coverage Distribution**:
   - Average: ‚â•95%
   - Minimum: ‚â•80%
   - Standard deviation: <10%

2. **Accuracy** (on 10 PDFs with ground truth):
   - Field-level accuracy: ‚â•95%
   - Numeric accuracy: ‚â•98%
   - Evidence accuracy: ‚â•90%

3. **Performance**:
   - Average time: <200s/PDF
   - Average cost: <$0.20/PDF
   - Success rate: ‚â•98%

4. **Failure Analysis**:
   - Categorize all failures (<80% coverage)
   - Identify common patterns
   - Implement additional fixes if needed

### Validation Criteria

**PASS** if:
- Average coverage ‚â•95%
- Accuracy (on ground truth) ‚â•95%
- Success rate ‚â•98%
- No catastrophic failures (<50% coverage)

**ITERATE** if:
- Average coverage 90-95%: Minor fixes needed
- Average coverage <90%: Major revision needed

---

## üöÄ Phase 9: Production Deployment (27,000 PDFs)

**Timeline**: 2-3 days
**Expected Impact**: Full corpus extraction
**Priority**: **P0 - PRODUCTION**

### Infrastructure Setup

#### Parallel Processing Architecture

```python
# 50 parallel workers
from multiprocessing import Pool
from tqdm import tqdm

def process_pdf_batch(pdf_paths: List[str], worker_id: int):
    """Process a batch of PDFs in parallel"""
    pipeline = OptimalBRFPipeline(
        enable_caching=True,
        output_dir=f"results/production/worker_{worker_id}"
    )

    results = []
    for pdf_path in tqdm(pdf_paths, desc=f"Worker {worker_id}"):
        try:
            result = pipeline.extract_document(pdf_path)
            results.append({
                "pdf": pdf_path,
                "status": "success",
                "coverage": result.coverage_pct
            })
        except Exception as e:
            results.append({
                "pdf": pdf_path,
                "status": "failed",
                "error": str(e)
            })

    return results

# Distribute 27,000 PDFs across 50 workers
num_workers = 50
pdfs_per_worker = 27000 // num_workers  # 540 PDFs/worker

with Pool(num_workers) as pool:
    all_results = pool.starmap(
        process_pdf_batch,
        [(batch, worker_id) for worker_id, batch in enumerate(pdf_batches)]
    )
```

### Cost & Performance Projections

| Metric | Estimate | Calculation |
|--------|----------|-------------|
| **Total PDFs** | 27,000 | Full √Örsredovisning corpus |
| **Processing Time** | **13.5 hours** | 27,000 PDFs √∑ 50 workers √∑ 0.67 PDFs/min |
| **Total Cost** | **$3,780** | 27,000 PDFs √ó $0.14/PDF |
| **Success Rate** | **98%** | Expected 26,460 successful |
| **Average Coverage** | **95%** | Validated on 100-PDF test |

### Monitoring & Quality Assurance

#### Real-time Monitoring Dashboard

```python
# Track metrics during processing
metrics = {
    "completed": 0,
    "failed": 0,
    "avg_coverage": 0.0,
    "avg_time": 0.0,
    "total_cost": 0.0,
    "low_coverage_count": 0,  # <80%
    "errors_by_type": {}
}

# Update every 100 PDFs
# Alert if:
# - Failure rate >5%
# - Average coverage <90%
# - Cost per PDF >$0.25
```

#### Quality Validation Sample

```python
# Validate random sample every 1,000 PDFs
sample_size = 100
random_sample = random.sample(completed_pdfs, sample_size)

validation_results = validate_against_ground_truth(random_sample)

if validation_results['accuracy'] < 0.95:
    print("‚ö†Ô∏è ALERT: Quality degradation detected!")
    # Pause processing and investigate
```

### Error Recovery Strategy

```python
# Automatic retry with exponential backoff
def process_with_retry(pdf_path, max_retries=3):
    for attempt in range(max_retries):
        try:
            return pipeline.extract_document(pdf_path)
        except Exception as e:
            if attempt == max_retries - 1:
                # Log failure and continue
                log_failure(pdf_path, e)
                return None
            time.sleep(2 ** attempt)  # 1s, 2s, 4s
```

### Post-Processing & Validation

```python
# After all PDFs processed:
1. Generate comprehensive report
2. Validate coverage distribution
3. Identify systematic failures
4. Create remediation task list for failed PDFs
5. Export to production database
```

---

## üìã Roadmap Timeline Summary

| Phase | Timeline | Expected Coverage | Status |
|-------|----------|-------------------|--------|
| **Phase 4B** | **Complete** | **84%** | ‚úÖ **DONE** |
| **Phase 5** | 2-3 hours | **89-92%** (+5-8 points) | üîÑ Next |
| **Phase 6** | 3-4 hours | **91-96%** (+2-4 points) | üîÑ Planned |
| **Phase 7** | 2-3 hours | **93-99%** (+2-3 points) | üîÑ Planned |
| **Phase 8** | 1 day | **95%** (validation) | üîÑ Planned |
| **Phase 9** | 2-3 days | **95%** (production) | üîÑ Planned |

**Total Time to Production**: **4-5 days** (including testing)

---

## üéØ Success Criteria

### Phase 5 Success
- [ ] Property coverage: 0% ‚Üí 60%+
- [ ] Operations coverage: 0% ‚Üí 40%+
- [ ] Average coverage: 84% ‚Üí 89-92%
- [ ] No regression on existing agents

### Phase 6 Success
- [ ] Note detection: 3 ‚Üí 15-20 notes/PDF
- [ ] Note coverage: 60% ‚Üí 85%+
- [ ] Average coverage: 89-92% ‚Üí 91-96%

### Phase 7 Success
- [ ] Outlier coverage: 71.7% ‚Üí 85%+
- [ ] All 10 Phase 4 PDFs tested
- [ ] Average coverage: 91-96% ‚Üí 93-99%

### Phase 8 Success
- [ ] 100-PDF average: ‚â•95%
- [ ] Ground truth accuracy: ‚â•95%
- [ ] Success rate: ‚â•98%
- [ ] No catastrophic failures

### Phase 9 Success
- [ ] 27,000 PDFs processed
- [ ] Average coverage: ‚â•95%
- [ ] Total cost: <$5,000
- [ ] Processing time: <24 hours

---

## üö® Risk Mitigation

### Risk 1: Coverage Doesn't Reach 95%
**Mitigation**: Phase 8 validation catches this early; iterate on Phases 5-7 before production

### Risk 2: Cost Exceeds Budget
**Mitigation**: Monitor cost per PDF; optimize high-cost outliers; consider hybrid approach

### Risk 3: Processing Time Too Long
**Mitigation**: Increase workers to 100 (halves time to 6-7 hours); optimize caching

### Risk 4: Quality Degradation at Scale
**Mitigation**: Random sampling validation every 1,000 PDFs; pause if quality drops

---

## üìä Expected Final Metrics

**After Phase 9 Completion**:

| Metric | Target | Expected | Confidence |
|--------|--------|----------|------------|
| **Coverage** | ‚â•95% | **95-97%** | **High** ‚úÖ |
| **Accuracy** | ‚â•95% | **95-98%** | **High** ‚úÖ |
| **Success Rate** | ‚â•95% | **98%** | **High** ‚úÖ |
| **Cost/PDF** | <$0.20 | **$0.14** | **High** ‚úÖ |
| **Time/PDF** | <300s | **165-200s** | **Medium** ‚ö†Ô∏è |
| **Total Cost** | <$5,000 | **$3,780** | **High** ‚úÖ |
| **Total Time** | <24h | **13.5h** | **Medium** ‚ö†Ô∏è |

---

**Next Step**: Execute Phase 5 (Property/Operations Cross-Extraction)
**ETA to Production**: **4-5 days**
**Confidence Level**: **HIGH** üöÄ
