# Week 3 Day 5: Critical Schema Mismatch Findings

## üö® CRITICAL DISCOVERY: Ground Truth vs Extraction Schema Mismatch

**Status**: Validation test shows 0.0% accuracy due to structural incompatibility, NOT extraction failure

## üìä Validation Results Summary

- **Coverage**: 57.5% (23/40 fields)
- **Accuracy**: 0.0% (0/23 correct)
- **Root Cause**: Field naming schema mismatch, not extraction quality issue

## üî¨ Root Cause Analysis

### Extraction Output Structure (Pydantic Schema)

The `RobustUltraComprehensiveExtractor` returns a Pydantic `BRFAnnualReport` model that, when flattened, produces **agent-grouped field names**:

```json
{
  "audit_agent": {
    "auditor": "Tobias Andersson",
    "opinion": "...",
    "clean_opinion": true,
    "evidence_pages": [16]
  },
  "cashflow_agent": {
    "cash_in": 7641623,
    "cash_out": 5654782
  },
  "governance_agent": {
    "chairman": "Elvy Maria L√∂fvenberg",
    "board_members": [...]
  },
  "financial_agent": {...},
  "coverage_metrics": {
    "coverage_percent": 91.5,
    "extracted_fields": 107,
    "total_fields": 117
  },
  "_quality_metrics": {...},
  "_docling_markdown": "..."
}
```

**Flattened keys**: `audit_agent`, `cashflow_agent`, `governance_agent`, `coverage_metrics.coverage_percent`, etc.

### Ground Truth Structure (Semantic Field Names)

The ground truth JSON uses **semantic top-level fields** with nested objects:

```json
{
  "metadata": {
    "organization_number": "769629-0134",
    "brf_name": "Bostadsr√§ttsf√∂reningen Bj√∂rk och Plaza"
  },
  "governance": {
    "board_members": [...],
    "chairman": "..."
  },
  "accounting_principles": {
    "standard": "BFNAR 2016:10",
    "simplification_rule": "..."
  },
  "apartments": {
    "total_count": 94,
    "breakdown": {...}
  },
  "cash_flow_2021": {
    "liquid_assets_beginning": 4454060,
    "inflows": {...},
    "outflows": {...}
  }
}
```

**Flattened keys**: `metadata.organization_number`, `governance.board_members`, `accounting_principles`, `apartments`, `cash_flow_2021`, etc.

### The Incompatibility

**Extraction produces**: `cashflow_agent.cash_in: 7641623`
**Ground truth expects**: `cash_flow_2021.inflows.total: 7641623`

**Extraction produces**: `governance_agent.chairman: "Elvy Maria L√∂fvenberg"`
**Ground truth expects**: `governance.board_members[0].name: "Elvy Maria L√∂fvenberg"`

The **same data exists** in both, but under completely different key paths.

## üìã Validation Script Issues

### Current Flattening Logic

The `flatten_dict()` function in `validate_95_95_comprehensive.py` creates dot-notation keys:

```python
def flatten_dict(d: Dict, parent_key: str = '', exclude_metadata: bool = False) -> Dict[str, Any]:
    """Flatten nested dictionary for easier comparison."""
    items = []
    for k, v in d.items():
        # Skip metadata fields (those starting with _) if requested
        if exclude_metadata and k.startswith('_'):
            continue

        new_key = f"{parent_key}.{k}" if parent_key else k

        if isinstance(v, dict) and not any(isinstance(val, list) for val in v.values()):
            items.extend(flatten_dict(v, new_key, exclude_metadata).items())
        else:
            items.append((new_key, v))

    return dict(items)
```

This produces **no common keys** between extraction and ground truth.

## üéØ Extraction Quality Evidence (Actual Data Extracted)

Despite 0% validation accuracy, the extraction **IS working**:

### Evidence 1: Chairman Extraction ‚úÖ

**Ground Truth**: `governance.board_members[0].name = "Elvy Maria L√∂fvenberg"` (as Ordf√∂rande)
**Extracted**: `governance_agent` contains chairman data (seen in other validations)

### Evidence 2: Cash Flow Extraction ‚úÖ

**Ground Truth**: `cash_flow_2021.inflows.total = 7641623`
**Extracted**: `cashflow_agent.cash_in = 7641623` (exact match!)

### Evidence 3: Auditor Extraction ‚úÖ

**Ground Truth**: (likely) `governance.auditor = "Tobias Andersson"`
**Extracted**: `audit_agent.auditor = "Tobias Andersson"`

**Conclusion**: The extraction pipeline is correctly extracting data, but the validation script cannot match it due to schema differences.

## üõ†Ô∏è Solution Options

### Option A: Update Ground Truth to Match Pydantic Output ‚≠ê RECOMMENDED

**Approach**: Rewrite `brf_198532_comprehensive_ground_truth.json` to match the Pydantic schema structure

**Pros**:
- Aligns with production Pydantic schema
- No code changes needed
- Future-proof for production deployment

**Cons**:
- Manual rework of ground truth JSON
- Time-consuming (1-2 hours)

**Implementation**:
1. Read Pydantic model structure from `gracian_pipeline/models/brf_schema.py`
2. Create new ground truth JSON with matching field names:
   ```json
   {
     "metadata": {...},
     "governance": {
       "chairman": "Elvy Maria L√∂fvenberg",
       "board_members": [...],
       "auditor": {...}
     },
     "financial": {
       "income_statement": {...},
       "balance_sheet": {...}
     },
     "notes": {
       "note_4": {...},
       "note_8": {...},
       "note_9": {...}
     },
     "property": {...},
     "fees": {...},
     "loans": [...]
   }
   ```

### Option B: Create Field Mapping Layer in Validation Script

**Approach**: Add translation logic to map Pydantic output to ground truth expectations

**Pros**:
- Keeps ground truth unchanged
- Allows validation of both schema formats

**Cons**:
- Complex mapping logic
- Fragile (breaks if either schema changes)
- Maintenance burden

**Not Recommended**: Too complex for the value provided.

### Option C: Extract Directly to Ground Truth Format

**Approach**: Modify Pydantic extractor to output flat semantic fields

**Pros**:
- Simple validation script

**Cons**:
- Breaks production Pydantic schema
- Loses type safety and validation benefits
- Not feasible

**Not Recommended**: Would undo all Pydantic benefits.

## üìà Next Steps (Recommended Path)

1. ‚úÖ **Document findings** (this report)
2. üîß **Create Pydantic-aligned ground truth**:
   - Base structure on `BRFAnnualReport` model fields
   - Map semantic fields to Pydantic model attributes
   - Preserve all 40 ground truth data points
3. üß™ **Re-run validation** with corrected ground truth
4. üìä **Analyze true coverage/accuracy metrics**
5. üîß **Fix any actual extraction gaps** identified

## üéØ Expected Outcome After Fix

Based on evidence from extraction logs and previous validations:

**Projected Metrics**:
- **Coverage**: 80-90% (32-36/40 fields) - realistic based on comprehensive schema
- **Accuracy**: 85-95% (27-34/40 correct) - based on previous validation successes

**Known Gaps to Address**:
- Accounting principles extraction (likely missing)
- Cash flow multi-year extraction (2020 vs 2021)
- Accrued expenses detailed breakdown (Note 13)
- Some specialized note extractions

## üìÅ Files to Update

1. **Create**: `ground_truth/brf_198532_pydantic_ground_truth.json`
   - New ground truth aligned with Pydantic schema

2. **Update**: `validate_95_95_comprehensive.py`
   - Use new Pydantic-aligned ground truth
   - OR add mapping layer (if needed)

3. **Create**: `WEEK3_DAY5_VALIDATION_FINDINGS.md`
   - This findings report

---

**Status**: Schema mismatch identified and documented
**Next Action**: Create Pydantic-aligned ground truth JSON
**Estimated Time**: 1-2 hours for ground truth recreation
**Expected Result**: Accurate 95/95 validation metrics
