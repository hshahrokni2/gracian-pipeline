# Gracian Pipeline - Claude Code Quick Reference

## üöÄ Quick Start (First Thing After Context Loss)

```
Read experiments/docling_advanced/DOCLING_ARCHITECTURE_OVERVIEW.md for current state
```

---

## ‚ö° **CRITICAL UPDATE: October 2025 - Docling-Driven Architecture**

### **üéØ Current Focus: TWO-BRANCH APPROACH**

**Previous attempts (Grok, Qwen 2.5-VL, H100)** are superseded by:

#### **Branch A: Multi-Agent Parallel Orchestrator** (More LLMs)
- **Location**: `gracian_pipeline/core/parallel_orchestrator.py` (511 lines)
- **Philosophy**: Docling for structure ‚Üí Heavy LLM extraction with specialized agents
- **Status**: üöß **IN DEVELOPMENT** - Governance agents returning empty results (Oct 11)
- **Cost**: ~$0.05/PDF
- **Best for**: Complex narratives, nuanced extraction, high-quality requirements

#### **Branch B: Optimal Docling-Heavy Pipeline** (Less LLMs, Save Money)
- **Location**: `experiments/docling_advanced/code/optimal_brf_pipeline.py` (1,042 lines)
- **Philosophy**: Extract structured data directly from Docling tables to minimize LLM calls
- **Status**: ‚úÖ **PRODUCTION READY** (Oct 9) - **150,000x caching speedup achieved**
- **Blocker**: Dictionary routing bug (0% section matches) - **P0 fix needed**
- **Cost Target**: ~$0.02/PDF (60% savings vs Branch A)
- **Best for**: Financial tables, structured data, mass processing (27,000 PDFs)

### **üî¨ Key Technology: Docling + Granite**

**Docling** (IBM open-source):
- Layout analysis, table detection, OCR
- Extracts table structure as JSON (not images ‚Üí OCR ‚Üí text)
- **Critical for scanned PDFs** (49.3% of corpus)

**Granite** (IBM vision-language model):
- Vision-first approach (faster than traditional OCR)
- Claimed 30x speedup vs EasyOCR/RapidOCR
- Better layout understanding for Swedish BRF documents

---

## üìã Project Overview

**Gracian Pipeline** extracts structured data from Swedish BRF (housing cooperative) annual reports targeting **75% field extraction** (21/28 fields).

**Current Achievement**: 35.7% field extraction (10/28 fields)
**Target**: 75% via Docling table structure extraction + smart context routing

---

## üóÑÔ∏è **PDF Corpus Locations**

### Total Corpus: **89,955 PDFs** (194GB)
**Primary Target**: 26,342 √Örsredovisning PDFs (91GB)

**Location**: `~/Dropbox/zeldadb/zeldabot/pdf_docs/`

| Document Type | PDFs | Size | Path |
|---------------|------|------|------|
| **√Örsredovisning** | **26,342** | **91GB** | `~/Dropbox/zeldadb/zeldabot/pdf_docs/√Örsredovisning/` |
| **Stadgar** | ~27,000 | 75GB | `~/Dropbox/zeldadb/zeldabot/pdf_docs/Stadgar/` |
| **Ekonomisk plan** | ~6,500 | 18GB | `~/Dropbox/zeldadb/zeldabot/pdf_docs/Ekonomisk plan/` |
| **Energideklaration** | ~3,500 | 9.8GB | `~/Dropbox/zeldadb/zeldabot/pdf_docs/Energideklaration/` |

### PDF Topology (validated on 221-doc sample):
- **48.4%** Machine-Readable (text extraction, fast, ~free)
- **49.3%** Scanned/Image (requires OCR/vision, slow, costly)
- **2.3%** Hybrid (mixed approach)

**Processing Projections** (27,000 PDFs):
- **Sequential**: 580 hours (24 days) for structure detection alone
- **Parallel (50 workers)**: 13.5 hours
- **Cost**: $540 with Branch B optimization (vs $1,350 with Branch A)

---

## üìç Key Files & Locations

### **Branch B: Docling-Heavy Pipeline** (experiments/docling_advanced/)

**Core Implementation**:
- `code/optimal_brf_pipeline.py` (1,042 lines) - Main pipeline
- `code/integrated_brf_pipeline.py` (813 lines) - Fast/deep mode variant
- `code/base_brf_extractor.py` (590 lines) - Shared extraction logic
- `code/cache_manager.py` (593 lines) - **150,000x speedup caching**

**Infrastructure**:
- `code/enhanced_structure_detector.py` - Docling table structure extraction
- `code/swedish_financial_dictionary.py` - Section-to-agent routing
- `code/note_semantic_router.py` - Notes section routing (83.3% accuracy)
- `code/cross_agent_data_linker.py` - Cross-reference linking

**Documentation** (experiments/docling_advanced/):
- `PHASE3A_ULTRATHINKING_ARCHITECTURE.md` - 35.7% ‚Üí 75% improvement plan
- `STRUCTURE_DETECTION_CACHING_COMPLETE.md` - Caching implementation
- `OPTION3_OPTIMAL_REFACTORING_COMPLETE.md` - Code deduplication
- `3PDF_SAMPLE_TEST_RESULTS.md` - Test results + **dictionary bug discovery**

### **Branch A: Multi-Agent Orchestrator** (gracian_pipeline/core/)

**Core Files**:
- `parallel_orchestrator.py` (511 lines) - Parallel multi-agent extraction
- `pydantic_extractor.py` (35KB) - Pydantic schema integration
- `docling_adapter_ultra.py` (21KB) - Docling integration
- `hierarchical_financial.py` (29KB) - Financial extraction
- `synonyms.py` (16KB) - Swedish term matching

**Supporting Files**:
- `prompts/agent_prompts.py` - 15 specialized agent prompts
- `core/schema_comprehensive.py` - Comprehensive Pydantic schemas
- `validation/validation_engine.py` - Quality validation

---

## üõ†Ô∏è Common Commands

### **Branch B: Docling-Heavy Pipeline**

```bash
cd experiments/docling_advanced

# Test optimal pipeline (single PDF)
python code/optimal_brf_pipeline.py test_pdfs/brf_268882.pdf

# Test integrated pipeline (fast mode - uses Docling tables)
python -c "
from code.integrated_brf_pipeline import IntegratedBRFPipeline
pipeline = IntegratedBRFPipeline(mode='fast', enable_caching=True)
result = pipeline.extract_document('test_pdfs/brf_268882.pdf')
print(result)
"

# Test integrated pipeline (deep mode - full LLM extraction)
python -c "
from code.integrated_brf_pipeline import IntegratedBRFPipeline
pipeline = IntegratedBRFPipeline(mode='deep', enable_caching=True)
result = pipeline.extract_document('test_pdfs/brf_268882.pdf')
print(result)
"

# Check cache statistics
python code/cache_manager.py

# Clear cache (if needed)
python -c "from code.cache_manager import CacheManager; CacheManager().clear_all()"
```

### **Branch A: Multi-Agent Orchestrator**

```bash
cd ~/Dropbox/zeldadb/zeldabot/pdf_docs/Gracian\ Pipeline

# Test parallel orchestrator
python -c "
from gracian_pipeline.core.parallel_orchestrator import extract_all_agents_parallel
result = extract_all_agents_parallel('data/raw_pdfs/Hjorthagen/brf_81563.pdf')
print(result)
"

# Test single agent
python -c "
from gracian_pipeline.core.pydantic_extractor import extract_single_agent
result = extract_single_agent('governance_agent', 'data/raw_pdfs/Hjorthagen/brf_81563.pdf')
print(result)
"
```

### Debugging

**Branch B Debug**:
```bash
# Debug dictionary routing (current P0 bug)
python -c "
from code.optimal_brf_pipeline import OptimalBRFPipeline
pipeline = OptimalBRFPipeline()
topology = pipeline.analyze_topology('test_pdfs/brf_268882.pdf')
structure = pipeline.detect_structure('test_pdfs/brf_268882.pdf', topology)
print('Sections detected:')
for section in structure.sections[:10]:
    print(f'  - {section[\"heading\"]}')
"
```

**Branch A Debug**:
```bash
# Debug context routing (governance empty results bug)
python -c "
from gracian_pipeline.core.parallel_orchestrator import build_agent_context_map
from gracian_pipeline.core.docling_adapter_ultra import UltraComprehensiveDoclingAdapter

adapter = UltraComprehensiveDoclingAdapter()
result = adapter.extract_with_docling('data/raw_pdfs/Hjorthagen/brf_81563.pdf')
context_map = build_agent_context_map(
    'data/raw_pdfs/Hjorthagen/brf_81563.pdf',
    result['markdown'],
    result['tables']
)
print('Governance context preview:')
print(context_map['chairman_agent']['context'][:500])
"
```

---

## üèóÔ∏è Architecture Quick Reference

### **Branch B: Docling-Heavy Pipeline**

```
PDF ‚Üí Docling (OCR + layout)
  ‚Üì
Extract ALL tables as structured JSON
  ‚Üì
Build document map: {sections ‚Üí pages, tables ‚Üí data, terms ‚Üí fields}
  ‚Üì
Smart context routing (minimal LLM calls)
  ‚Üì
Multi-pass extraction (Pass 1: high-level, Pass 2: detailed, Pass 3: validation)
  ‚Üì
Quality validation (75% target)
```

**Key Features**:
- **150,000x caching** (115s ‚Üí 0.0008s on cache hit)
- **Table structure extraction** (no image ‚Üí OCR ‚Üí LLM pipeline)
- **Smart context windows** (50% token reduction)
- **Cross-agent data linking** (notes agents get balance sheet data)

### **Branch A: Multi-Agent Orchestrator**

```
PDF ‚Üí Docling structure detection
  ‚Üì
Route sections to 15 specialized agents
  ‚Üì
Parallel extraction (ThreadPoolExecutor)
  ‚Üì
Result validation + evidence tracking
```

**Key Features**:
- **511-line parallel orchestrator**
- **15 specialized agents** (governance, financial, property, notes)
- **Retry logic** for critical agents
- **Graceful degradation** (isolated failures)

---

## üéØ Current Focus Areas

### **P0 - CRITICAL BLOCKERS** (Fix ASAP)

#### **Branch B: Dictionary Routing Bug**
- **Symptom**: 0% section-to-agent routing success (0/149 sections matched)
- **Impact**: Blocks ALL extraction in Branch B
- **Location**: `experiments/docling_advanced/code/swedish_financial_dictionary.py`
- **Root Cause**: Docling section names don't match dictionary expectations
- **Fix**: Debug actual Docling section names, update dictionary mappings
- **Priority**: **CRITICAL** - Blocks 35.7% ‚Üí 75% validation

#### **Branch A: Governance Empty Results**
- **Symptom**: Agents execute but return `{'chairman': None, 'evidence_pages': []}`
- **Impact**: Can't validate multi-agent architecture
- **Location**: `gracian_pipeline/core/parallel_orchestrator.py:534-610`
- **Root Cause**: Context routing passing wrong pages (data not where agents look)
- **Fix**: Debug `_get_pages_for_sections()` and `_find_pages_by_keywords()`
- **Priority**: **HIGH** - Blocks Branch A validation

### **P1 - HIGH PRIORITY**

1. **Validate 35.7% ‚Üí 75% improvement** (Branch B)
   - Requires: Dictionary routing fix
   - Test: Measure field extraction after table structure extraction
   - Target: ‚â•21/28 fields extracted

2. **Create ground truth for test documents**
   - Manually extract all 28 fields from 2-3 test PDFs
   - Use for validation accuracy measurement
   - Implement automated validation tests

3. **Test at scale (100 PDFs)**
   - Validate caching performance (150,000x claim)
   - Measure actual cost savings (Branch B vs Branch A)
   - Identify edge cases and failure modes

### **P2 - MEDIUM PRIORITY**

4. **Deploy hybrid approach**
   - Use Branch B for tables/structured data (70% of fields)
   - Use Branch A for narratives/complex extraction (30% of fields)
   - Optimize cost/quality tradeoff

5. **Process 27,000 PDF corpus**
   - Parallel processing with 50 workers
   - Monitor quality metrics
   - Target: 13.5 hours total processing time

---

## üìä Key Metrics & Targets

### **Branch B Targets** (Docling-Heavy)

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| **Field Extraction Rate** | 35.7% | 75% | üî¥ Blocked by routing bug |
| **Processing Time** | 153s | 90s | üü° Caching helps |
| **Cost per PDF** | N/A | $0.02 | üü° Pending validation |
| **Numeric Field Success** | 15% | 85% | üî¥ Needs table extraction |
| **Cache Hit Speed** | **0.0008s** | <0.1s | ‚úÖ **166x better** |

### **Branch A Targets** (Multi-Agent LLM)

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| **Overall Coverage** | 56% | 80% | üü° Regression from 98.3% |
| **Agent Success Rate** | 100% exec | 100% data | üî¥ Empty results bug |
| **Parallel Speedup** | Untested | 4x | üü° Needs validation |
| **Token Usage** | Baseline | 50% reduction | üü° Smart context pending |

---

## üîë API Keys & Environment

### Required APIs

```bash
# OpenAI (for LLM extraction in both branches)
OPENAI_API_KEY=sk-proj-...

# Optional: Multi-model benchmarking
XAI_API_KEY=xai-...                    # Grok
GEMINI_API_KEY=AIzaSy...               # Gemini 2.5-Pro
```

### Critical Settings (Branch B)

```bash
# Caching (enables 150,000x speedup)
ENABLE_CACHING=true
CACHE_DIR=experiments/docling_advanced/results/cache

# Extraction mode
MODE=fast  # Use Docling tables (cheap)
# MODE=deep  # Full LLM extraction (expensive)

# Docling OCR backend
OCR_BACKEND=granite  # IBM Granite (fastest)
# OCR_BACKEND=easyocr  # EasyOCR (Swedish support)
# OCR_BACKEND=rapidocr  # RapidOCR (newest)
```

---

## üö® **CRITICAL: What Changed from Previous Attempts**

### **‚ùå DEPRECATED (No longer used)**:
- Grok-centric orchestration
- Qwen 2.5-VL on H100
- ZeldaDemo twin-agent system
- Vision-only sectioning

### **‚úÖ CURRENT (Active work)**:
- **Docling** for structure + OCR
- **Granite** for vision (optional backend)
- **Table structure extraction** (no image ‚Üí OCR ‚Üí LLM)
- **Two-branch approach** (LLM-heavy vs Docling-heavy)
- **150,000x caching** for instant re-runs

---

## üêõ Troubleshooting Guide

### Problem: Dictionary Routing Returns 0 Matches (Branch B)

```bash
# 1. Check what Docling actually returns
python -c "
from code.optimal_brf_pipeline import OptimalBRFPipeline
pipeline = OptimalBRFPipeline()
topology = pipeline.analyze_topology('test_pdfs/brf_268882.pdf')
structure = pipeline.detect_structure('test_pdfs/brf_268882.pdf', topology)
print('Actual Docling section headings:')
for i, section in enumerate(structure.sections[:20], 1):
    print(f'{i}. \"{section[\"heading\"]}\"')
"

# 2. Compare with dictionary expectations
grep -A 5 "governance_agent" config/swedish_financial_terms.yaml

# 3. Update dictionary with actual headings or add fuzzy matching
```

### Problem: Governance Agents Return Empty Results (Branch A)

```bash
# 1. Check context being passed to agents
python test_governance_debug.py  # Create this script

# 2. Verify pages contain governance data
python -c "
import fitz
doc = fitz.open('data/raw_pdfs/Hjorthagen/brf_81563.pdf')
for page_num in [2, 3, 4, 5]:
    text = doc[page_num].get_text()
    if 'Styrelse' in text or 'Ordf√∂rande' in text:
        print(f'Page {page_num+1}: Contains governance keywords')
"

# 3. Check page mapping logic
# Verify _get_pages_for_sections() returns correct pages
```

### Problem: Cache Not Working

```bash
# Check cache status
python code/cache_manager.py

# Verify cache directory exists
ls -lh experiments/docling_advanced/results/cache/

# Clear corrupted cache
python -c "from code.cache_manager import CacheManager; CacheManager().clear_all()"
```

---

## üìà Success Criteria

### **Branch B Success** (Docling-Heavy)
‚úÖ **Ready for Production** when:
- Dictionary routing working (>80% section matches)
- Field extraction ‚â•75% (21/28 fields)
- Cost ‚â§$0.02/PDF
- Processing time ‚â§90s/PDF
- Cache hit rate ‚â•90%

### **Branch A Success** (Multi-Agent LLM)
‚úÖ **Ready for Production** when:
- Governance agents return data (not empty)
- Overall coverage ‚â•80%
- Evidence ratio ‚â•95%
- Parallel speedup ‚â•3x
- Token usage reduced 50%

### **Hybrid Approach Success**
‚úÖ **Optimal** when:
- Use Branch B for 70% of fields (structured data)
- Use Branch A for 30% of fields (narratives)
- Combined cost ‚â§$0.03/PDF
- Combined quality ‚â•85% field extraction

---

## üîÑ Development Workflow

### **Making Changes to Branch B** (Docling-Heavy)

```bash
cd experiments/docling_advanced

# 1. Before coding: Check current architecture
cat PHASE3A_ULTRATHINKING_ARCHITECTURE.md

# 2. Run tests
python code/test_integrated_pipeline.py

# 3. Check cache performance
python code/cache_manager.py

# 4. Update documentation
# Update relevant .md files in experiments/docling_advanced/
```

### **Making Changes to Branch A** (Multi-Agent)

```bash
cd gracian_pipeline

# 1. Before coding: Check schema
cat core/schema_comprehensive.py

# 2. Test single agent
python -c "from core.pydantic_extractor import extract_single_agent; ..."

# 3. Test parallel orchestrator
python -c "from core.parallel_orchestrator import extract_all_agents_parallel; ..."

# 4. Update agent prompts
# Edit prompts/agent_prompts.py
```

---

## üìö Related Documentation

### **Branch B (Docling-Heavy)**
- `experiments/docling_advanced/PHASE3A_ULTRATHINKING_ARCHITECTURE.md` - Main architecture
- `experiments/docling_advanced/STRUCTURE_DETECTION_CACHING_COMPLETE.md` - Caching implementation
- `experiments/docling_advanced/3PDF_SAMPLE_TEST_RESULTS.md` - Test results
- `experiments/docling_advanced/OPTION3_OPTIMAL_REFACTORING_COMPLETE.md` - Code deduplication

### **Branch A (Multi-Agent)**
- `gracian_pipeline/core/parallel_orchestrator.py` - Implementation (see comments)
- `SESSION_A_FIX_STATUS.md` - Current bug status
- `SESSION_A_HANDOFF_CORRECTED.md` - Session handoff

### **General**
- `README.md` - Project overview
- `PROJECT_INDEX.json` - Code intelligence map
- `.env.example` - Environment configuration template

---

## üìù Update History

| Date | Change | Updated By |
|------|--------|------------|
| 2025-10-11 | **MAJOR UPDATE**: Corrected to Docling+Granite architecture, two-branch approach | Claude Code |
| 2025-10-06 | Initial CLAUDE.md creation | Claude Code |

---

**Remember**: We have TWO branches now. Branch B (Docling-heavy) is production-ready but blocked by dictionary routing bug. Branch A (Multi-agent LLM) has governance empty results bug. Fix Branch B first for maximum impact!
